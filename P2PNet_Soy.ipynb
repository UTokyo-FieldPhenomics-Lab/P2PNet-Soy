{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwBYMnSdD4hX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88638651-e911-4a5f-86bf-392f6dce5fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hdf5storage\n",
            "  Downloading hdf5storage-0.1.18-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.1 in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.1->hdf5storage) (1.5.2)\n",
            "Installing collected packages: hdf5storage\n",
            "Successfully installed hdf5storage-0.1.18\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[K     |█████████████▌                  | 834.1 MB 1.2 MB/s eta 0:15:51tcmalloc: large alloc 1147494400 bytes == 0x394e4000 @  0x7fafd03af615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |█████████████████               | 1055.7 MB 1.2 MB/s eta 0:12:43tcmalloc: large alloc 1434370048 bytes == 0x7db3a000 @  0x7fafd03af615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |█████████████████████▋          | 1336.2 MB 88.6 MB/s eta 0:00:08tcmalloc: large alloc 1792966656 bytes == 0x296c000 @  0x7fafd03af615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1 MB 1.2 MB/s eta 0:04:03tcmalloc: large alloc 2241208320 bytes == 0x6d754000 @  0x7fafd03af615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 1.1 MB/s eta 0:00:01tcmalloc: large alloc 1982251008 bytes == 0xf30b6000 @  0x7fafd03ae1e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9\n",
            "tcmalloc: large alloc 2477817856 bytes == 0x169322000 @  0x7fafd03af615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 5.3 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (17.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.6 MB 74.7 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.8.0\n",
            "  Downloading torchaudio-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (4.1.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.0+cu111) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.11.0+cu113\n",
            "    Uninstalling torchaudio-0.11.0+cu113:\n",
            "      Successfully uninstalled torchaudio-0.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.8.0+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu111 torchaudio-0.8.0 torchvision-0.9.0+cu111\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/nightly/cu101/torch-1.7.0.dev20200626%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install hdf5storage\n",
        "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install --pre torch -f  https://download.pytorch.org/whl/nightly/cu101/torch-1.7.0.dev20200626%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1laN8KjnD5Lv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69cc5b3-77f1-41be-d793-7fbcf4368268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2dc1-u6KMTS"
      },
      "outputs": [],
      "source": [
        "## \n",
        "import os\n",
        "import random\n",
        "from scipy import spatial\n",
        "import networkx as nx\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import glob\n",
        "import scipy.io as io\n",
        "from matplotlib import pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "\n",
        "class SHHA(Dataset):\n",
        "    def __init__(self, data_root, transform=None, train=False, patch=False, flip=False):\n",
        "        self.root_path = data_root\n",
        "        self.train_lists = os.path.join(self.root_path, \"soybean_seed_counting_a.txt\")\n",
        "        self.eval_list = os.path.join(self.root_path, \"soybean_seed_counting_b.txt\")\n",
        "        # \n",
        "        if train:\n",
        "            self.img_list_file = [name.split(',') for name in open(self.train_lists).read().splitlines()]\n",
        "        else:\n",
        "            self.img_list_file = [name.split(',') for name in open(self.eval_list).read().splitlines()]\n",
        "\n",
        "        self.img_list = self.img_list_file\n",
        "        \n",
        "        # \n",
        "        self.nSamples = len(self.img_list)\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "        self.patch = patch\n",
        "        self.flip = flip\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nSamples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        assert index <= len(self), 'index range error'\n",
        "\n",
        "        img_path = self.img_list[index][0]\n",
        "        gt_path = self.img_list[index][1]\n",
        "        # \n",
        "        img, point = load_data((img_path, gt_path), self.train)\n",
        "        #\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.train:\n",
        "            # data augmentation -> random scale\n",
        "            scale_range = [0.5, 1.4]\n",
        "            min_size = min(img.shape[1:])\n",
        "            scale = random.uniform(*scale_range)\n",
        "            # scale the image and points\n",
        "            if scale * min_size > 224:\n",
        "                img = torch.nn.functional.upsample_bilinear(img.unsqueeze(0), scale_factor=scale).squeeze(0)\n",
        "                point *= scale\n",
        "        # random crop augumentaiton\n",
        "        if self.train and self.patch:\n",
        "            img, point = random_crop(img, point)\n",
        "            for i, _ in enumerate(point):\n",
        "                point[i] = torch.Tensor(point[i])\n",
        "        # random flipping\n",
        "        if random.random() > 0.1 and self.train and self.flip: # never flip\n",
        "            # random flip\n",
        "            img = torch.Tensor(img[:, :, :, ::-1].copy())\n",
        "            for i, _ in enumerate(point):\n",
        "                point[i][:, 0] = 224 - point[i][:, 0]\n",
        "        # random change brightness\n",
        "        if random.random() > 0.3 and self.train: # never flip\n",
        "            #\n",
        "            img = (torch.Tensor(img).clone())*random.uniform(8,12)/10\n",
        "            for i, _ in enumerate(point):\n",
        "                point[i][:, 0] = point[i][:, 0]\n",
        "\n",
        "        if not self.train:\n",
        "            point = [point]\n",
        "\n",
        "        img = torch.Tensor(img)\n",
        "        #  need to adapt your own image names\n",
        "        target = [{} for i in range(len(point))]\n",
        "        for i, _ in enumerate(point):\n",
        "            target[i]['point'] = torch.Tensor(point[i])\n",
        "            image_id_1 = int(img_path.split('/')[-1].split('.')[0].split(\"_\")[1][4:8])\n",
        "            image_id_1 = torch.Tensor([image_id_1]).long()\n",
        "            #\n",
        "            image_id_2 = int(img_path.split('/')[-1].split('.')[0].split(\"_\")[3])\n",
        "            image_id_2 = torch.Tensor([image_id_2]).long()\n",
        "            target[i]['image_id_1'] = image_id_1\n",
        "            target[i]['image_id_2'] = image_id_2\n",
        "            target[i]['labels'] = torch.ones([point[i].shape[0]]).long()\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def load_data(img_gt_path, train):\n",
        "    img_path, gt_path = img_gt_path\n",
        "    # load the images\n",
        "    img = cv2.imread(img_path)\n",
        "    img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    # load ground truth points\n",
        "    points = []\n",
        "    #\n",
        "    pts = open(gt_path).read().splitlines()\n",
        "    for pt_0 in pts:\n",
        "        pt = eval(pt_0)        \n",
        "        x = float(pt[0])\n",
        "        y = float(pt[1])\n",
        "        points.append([x, y])\n",
        "    return img, np.array(points)\n",
        "\n",
        "# random crop augumentation\n",
        "def random_crop(img, den, num_patch=10):\n",
        "    half_h = 224\n",
        "    half_w = 224\n",
        "    result_img = np.zeros([num_patch, img.shape[0], half_h, half_w])\n",
        "    result_den = []\n",
        "    # \n",
        "    for i in range(num_patch):\n",
        "        start_h = random.randint(0, img.size(1) - half_h)\n",
        "        start_w = random.randint(0, img.size(2) - half_w)\n",
        "        end_h = start_h + half_h\n",
        "        end_w = start_w + half_w\n",
        "        # \n",
        "        result_img[i] = img[:, start_h:end_h, start_w:end_w]#*random.uniform(5,15)/10\n",
        "        # copy the cropped points\n",
        "        idx = (den[:, 0] >= start_w) & (den[:, 0] <= end_w) & (den[:, 1] >= start_h) & (den[:, 1] <= end_h)\n",
        "        # \n",
        "        record_den = den[idx]\n",
        "        record_den[:, 0] -= start_w\n",
        "        record_den[:, 1] -= start_h\n",
        "\n",
        "        result_den.append(record_den)\n",
        "\n",
        "    return result_img, result_den"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZHJfljJc-bb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fkDa6XEKMWE"
      },
      "outputs": [],
      "source": [
        "# \n",
        "import torchvision.transforms as standard_transforms\n",
        "\n",
        "# \n",
        "class DeNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m)\n",
        "        return tensor\n",
        "\n",
        "def loading_data(data_root):\n",
        "    # \n",
        "    transform = standard_transforms.Compose([\n",
        "        standard_transforms.ToTensor(), \n",
        "        standard_transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    # \n",
        "    train_set = SHHA(data_root, train=True, transform=transform, patch=True, flip=True)\n",
        "    # \n",
        "    val_set = SHHA(data_root, train=False, transform=transform)\n",
        "\n",
        "    return train_set, val_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q32CayMIjYwX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZY5NX0-KMZO"
      },
      "outputs": [],
      "source": [
        "# \n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from typing import Iterable\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import torchvision.transforms as standard_transforms\n",
        "import cv2\n",
        "\n",
        "class DeNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m)\n",
        "        return tensor\n",
        "#\n",
        "def vis(samples, targets, pred, vis_dir, epoch, predict_cnt, gt_cnt):\n",
        "    '''\n",
        "    samples -> tensor: [batch, 3, H, W]\n",
        "    targets -> list of dict: [{'points':[], 'image_id': str}]\n",
        "    pred -> list: [num_preds, 2]\n",
        "    '''\n",
        "    gts = [t['point'].tolist() for t in targets]\n",
        "\n",
        "    pil_to_tensor = standard_transforms.ToTensor()\n",
        "\n",
        "    restore_transform = standard_transforms.Compose([\n",
        "        DeNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        standard_transforms.ToPILImage()\n",
        "    ])\n",
        "    # \n",
        "    for idx in range(samples.shape[0]):\n",
        "        sample = restore_transform(samples[idx])\n",
        "        sample = pil_to_tensor(sample.convert('RGB')).numpy() * 255\n",
        "        sample_gt = sample.transpose([1, 2, 0])[:, :, :].astype(np.uint8).copy()\n",
        "        sample_pred = sample.transpose([1, 2, 0])[:, :, :].astype(np.uint8).copy()\n",
        "\n",
        "        max_len = np.max(sample_gt.shape)\n",
        "\n",
        "        size = 5\n",
        "        # draw gt\n",
        "        for t in gts[idx]:\n",
        "            sample_gt = cv2.circle(sample_gt, (int(t[0]), int(t[1])), size, (0, 255, 0), -1)\n",
        "        # draw predictions\n",
        "        for p in pred[idx]:\n",
        "            sample_pred = cv2.circle(sample_pred, (int(p[0]), int(p[1])), size, (0, 0, 255), -1)\n",
        "\n",
        "        name_1 = targets[idx]['image_id_1']\n",
        "        name_2 = targets[idx]['image_id_2']\n",
        "        #################\n",
        "        fig = plt.figure()\n",
        "        ax1 = fig.add_subplot(1, 2, 1)\n",
        "        ax1.imshow(sample_gt)\n",
        "        ax1.get_xaxis().set_visible(False)\n",
        "        ax1.get_yaxis().set_visible(False)\n",
        "        ax2 = fig.add_subplot(1, 2, 2)\n",
        "        ax2.imshow(sample_pred)\n",
        "        ax2.get_xaxis().set_visible(False)\n",
        "        ax2.get_yaxis().set_visible(False)\n",
        "        fig.suptitle('manual count=%4.2f, inferred count=%4.2f'%(gt_cnt, predict_cnt), fontsize=10)\n",
        "        plt.tight_layout(rect=[0, 0, 0.95, 0.95]) # maize tassels counting\n",
        "        plt.savefig(os.path.join(vis_dir, '{}_{}_id_{}_ind_{}.jpg'.format(epoch, idx, int(name_1), int(name_2))), bbox_inches='tight', dpi = 300)\n",
        "        plt.close()\n",
        "# the training\n",
        "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
        "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
        "                    device: torch.device, epoch: int, max_norm: float = 0):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    # iterate all training samples\n",
        "    for samples, targets in data_loader:\n",
        "        #\n",
        "        samples = samples.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        # forward\n",
        "        outputs = model(samples)\n",
        "        #\n",
        "        # calc the losses\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        weight_dict = criterion.weight_dict\n",
        "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "        # reduce all losses (get the mean values)\n",
        "        loss_dict_reduced = reduce_dict(loss_dict)\n",
        "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "                                      for k, v in loss_dict_reduced.items()}\n",
        "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
        "\n",
        "        loss_value = losses_reduced_scaled.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        if max_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "        # update logger\n",
        "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "\n",
        "# evaluate the model performance during training\n",
        "@torch.no_grad()\n",
        "def evaluate_crowd_no_overlap(model, data_loader, device, epoch, threshold, vis_dir=None):\n",
        "    model.eval()\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    # run inference on all images to calc MAE\n",
        "    maes = []\n",
        "    mses = []\n",
        "    for samples, targets in data_loader:\n",
        "\n",
        "        samples = samples.to(device)\n",
        "\n",
        "        outputs = model(samples)\n",
        "        outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]\n",
        "\n",
        "        outputs_points = outputs['pred_points'][0]\n",
        "\n",
        "        gt_cnt = targets[0]['point'].shape[0]\n",
        "        # 0.5 is used by default\n",
        "        threshold = threshold\n",
        "\n",
        "        points = outputs_points[outputs_scores > threshold].detach().cpu().numpy()#.tolist()\n",
        "        # choose to merge closely located points\n",
        "        if points.shape[0]<10000 and points.shape[0] != 0:\n",
        "            # choose the cut off point\n",
        "            cutoff = 500/points.shape[0]\n",
        "            if cutoff<20:\n",
        "                cutoff = 20\n",
        "            components = nx.connected_components(\n",
        "                nx.from_edgelist(\n",
        "                    (i, j) for i, js in enumerate(\n",
        "                        spatial.KDTree(points).query_ball_point(points, cutoff)\n",
        "                    )\n",
        "                    for j in js\n",
        "                )\n",
        "            )\n",
        "\n",
        "            clusters = {j: i for i, js in enumerate(components) for j in js}\n",
        "\n",
        "            # reorganize the points to the order of clusters \n",
        "            points_reo = np.zeros(points.shape)\n",
        "            i = 0\n",
        "            for key in clusters.keys():\n",
        "                points_reo[i,:] = points[key,:]\n",
        "                i+=1\n",
        "            # points_n has the same order as clusters\n",
        "            res = [clusters[key] for key in clusters.keys()]\n",
        "            res_n = np.array(res).reshape(-1,1)\n",
        "\n",
        "            points_n = []\n",
        "            for i in np.unique(res_n):\n",
        "                tmp = points_reo[np.where(res_n[:,0] == i)]\n",
        "                points_n.append( [np.mean(tmp[:,0]), np.mean(tmp[:,1])])\n",
        "        else:\n",
        "            points_n = points.tolist()\n",
        "\n",
        "        predict_cnt = len(points_n)\n",
        "        #save the visualized images\n",
        "        if vis_dir is not None: \n",
        "            vis(samples, targets, [points_n], vis_dir, epoch, predict_cnt, gt_cnt)\n",
        "        # accumulate MAE, MSE\n",
        "        mae = abs(predict_cnt - gt_cnt)\n",
        "        mse = (predict_cnt - gt_cnt) * (predict_cnt - gt_cnt)\n",
        "        maes.append(float(mae))\n",
        "        mses.append(float(mse))\n",
        "    # calc MAE, MSE\n",
        "    mae = np.mean(maes)\n",
        "    mse = np.sqrt(np.mean(mses))\n",
        "\n",
        "    return mae, mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GqceuB4CGnu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB4BETJp2m6W"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "from IPython.display import clear_output \n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "\n",
        "import os\n",
        "from tensorboardX import SummaryWriter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "from collections import defaultdict, deque\n",
        "import datetime\n",
        "import pickle\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch import Tensor\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# needed due to empty tensor bug in pytorch and torchvision 0.5\n",
        "import torchvision\n",
        "if float(torchvision.__version__[:3]) < 0.7:\n",
        "    from torchvision.ops import _new_empty_tensor\n",
        "    from torchvision.ops.misc import _output_size\n",
        "\n",
        "\n",
        "class SmoothedValue(object):\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median,\n",
        "            avg=self.avg,\n",
        "            global_avg=self.global_avg,\n",
        "            max=self.max,\n",
        "            value=self.value)\n",
        "#\n",
        "def all_gather(data):\n",
        "    \"\"\"\n",
        "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
        "    Args:\n",
        "        data: any picklable object\n",
        "    Returns:\n",
        "        list[data]: list of data gathered from each rank\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "\n",
        "    # serialized to a Tensor\n",
        "    buffer = pickle.dumps(data)\n",
        "    storage = torch.ByteStorage.from_buffer(buffer)\n",
        "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
        "\n",
        "    # obtain Tensor size of each rank\n",
        "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
        "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
        "    dist.all_gather(size_list, local_size)\n",
        "    size_list = [int(size.item()) for size in size_list]\n",
        "    max_size = max(size_list)\n",
        "\n",
        "    # receiving Tensor from all ranks\n",
        "    # we pad the tensor because torch all_gather does not support\n",
        "    # gathering tensors of different shapes\n",
        "    tensor_list = []\n",
        "    for _ in size_list:\n",
        "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
        "    if local_size != max_size:\n",
        "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
        "        tensor = torch.cat((tensor, padding), dim=0)\n",
        "    dist.all_gather(tensor_list, tensor)\n",
        "\n",
        "    data_list = []\n",
        "    for size, tensor in zip(size_list, tensor_list):\n",
        "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
        "        data_list.append(pickle.loads(buffer))\n",
        "\n",
        "    return data_list\n",
        "#\n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that all processes\n",
        "    have the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.no_grad():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.all_reduce(values)\n",
        "        if average:\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict\n",
        "#\n",
        "class MetricLogger(object):\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
        "            type(self).__name__, attr))\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(\n",
        "                \"{}: {}\".format(name, str(meter))\n",
        "            )\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = ''\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}',\n",
        "                'max mem: {memory:.0f}'\n",
        "            ])\n",
        "        else:\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}'\n",
        "            ])\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time),\n",
        "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
        "                else:\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time)))\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
        "            header, total_time_str, total_time / len(iterable)))\n",
        "#\n",
        "def get_sha():\n",
        "    cwd = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "    def _run(command):\n",
        "        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n",
        "    sha = 'N/A'\n",
        "    diff = \"clean\"\n",
        "    branch = 'N/A'\n",
        "    try:\n",
        "        sha = _run(['git', 'rev-parse', 'HEAD'])\n",
        "        subprocess.check_output(['git', 'diff'], cwd=cwd)\n",
        "        diff = _run(['git', 'diff-index', 'HEAD'])\n",
        "        diff = \"has uncommited changes\" if diff else \"clean\"\n",
        "        branch = _run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n",
        "    except Exception:\n",
        "        pass\n",
        "    message = f\"sha: {sha}, status: {diff}, branch: {branch}\"\n",
        "    return message\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = list(zip(*batch))\n",
        "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
        "    return tuple(batch)\n",
        "\n",
        "def collate_fn_crowd(batch):\n",
        "    # re-organize the batch\n",
        "    batch_new = []\n",
        "    for b in batch:\n",
        "        imgs, points = b\n",
        "        if imgs.ndim == 3:\n",
        "            imgs = imgs.unsqueeze(0)\n",
        "        for i in range(len(imgs)):\n",
        "            batch_new.append((imgs[i, :, :, :], points[i]))\n",
        "    batch = batch_new\n",
        "    batch = list(zip(*batch))\n",
        "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
        "    return tuple(batch)\n",
        "\n",
        "\n",
        "def _max_by_axis(the_list):\n",
        "    # type: (List[List[int]]) -> List[int]\n",
        "    maxes = the_list[0]\n",
        "    for sublist in the_list[1:]:\n",
        "        for index, item in enumerate(sublist):\n",
        "            maxes[index] = max(maxes[index], item)\n",
        "    return maxes\n",
        "\n",
        "def _max_by_axis_pad(the_list):\n",
        "    # type: (List[List[int]]) -> List[int]\n",
        "    maxes = the_list[0]\n",
        "    for sublist in the_list[1:]:\n",
        "        for index, item in enumerate(sublist):\n",
        "            maxes[index] = max(maxes[index], item)\n",
        "\n",
        "    block = 128\n",
        "\n",
        "    for i in range(2):\n",
        "        maxes[i+1] = ((maxes[i+1] - 1) // block + 1) * block\n",
        "    return maxes\n",
        "#\n",
        "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
        "    # TODO make this more general\n",
        "    if tensor_list[0].ndim == 3:\n",
        "\n",
        "        # TODO make it support different-sized images\n",
        "        max_size = _max_by_axis_pad([list(img.shape) for img in tensor_list])\n",
        "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
        "        batch_shape = [len(tensor_list)] + max_size\n",
        "        b, c, h, w = batch_shape\n",
        "        dtype = tensor_list[0].dtype\n",
        "        device = tensor_list[0].device\n",
        "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
        "        for img, pad_img in zip(tensor_list, tensor):\n",
        "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "    else:\n",
        "        raise ValueError('not supported')\n",
        "    return tensor\n",
        "#\n",
        "class NestedTensor(object):\n",
        "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
        "        self.tensors = tensors\n",
        "        self.mask = mask\n",
        "\n",
        "    def to(self, device):\n",
        "        # type: (Device) -> NestedTensor # noqa\n",
        "        cast_tensor = self.tensors.to(device)\n",
        "        mask = self.mask\n",
        "        if mask is not None:\n",
        "            assert mask is not None\n",
        "            cast_mask = mask.to(device)\n",
        "        else:\n",
        "            cast_mask = None\n",
        "        return NestedTensor(cast_tensor, cast_mask)\n",
        "\n",
        "    def decompose(self):\n",
        "        return self.tensors, self.mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.tensors)\n",
        "#\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    import builtins as __builtin__\n",
        "    builtin_print = __builtin__.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop('force', False)\n",
        "        if is_master or force:\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    __builtin__.print = print\n",
        "#\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "#\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "#\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "#\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "#\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "#\n",
        "def init_distributed_mode(args):\n",
        "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
        "        args.rank = int(os.environ[\"RANK\"])\n",
        "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
        "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
        "    elif 'SLURM_PROCID' in os.environ:\n",
        "        args.rank = int(os.environ['SLURM_PROCID'])\n",
        "        args.gpu = args.rank % torch.cuda.device_count()\n",
        "    else:\n",
        "        print('Not using distributed mode')\n",
        "        args.distributed = False\n",
        "        return\n",
        "\n",
        "    args.distributed = True\n",
        "\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "    args.dist_backend = 'nccl'\n",
        "    print('| distributed init (rank {}): {}'.format(\n",
        "        args.rank, args.dist_url), flush=True)\n",
        "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                         world_size=args.world_size, rank=args.rank)\n",
        "    torch.distributed.barrier()\n",
        "    setup_for_distributed(args.rank == 0)\n",
        "#\n",
        "@torch.no_grad()\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    if target.numel() == 0:\n",
        "        return [torch.zeros([], device=output.device)]\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "#\n",
        "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
        "    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n",
        "    \"\"\"\n",
        "    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n",
        "    This will eventually be supported natively by PyTorch, and this\n",
        "    class can go away.\n",
        "    \"\"\"\n",
        "    if float(torchvision.__version__[:3]) < 0.7:\n",
        "        if input.numel() > 0:\n",
        "            return torch.nn.functional.interpolate(\n",
        "                input, size, scale_factor, mode, align_corners\n",
        "            )\n",
        "\n",
        "        output_shape = _output_size(2, input, size, scale_factor)\n",
        "        output_shape = list(input.shape[:-2]) + list(output_shape)\n",
        "        return _new_empty_tensor(input, output_shape)\n",
        "    else:\n",
        "        return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)\n",
        "#\n",
        "class FocalLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "        This criterion is a implemenation of Focal Loss, which is proposed in\n",
        "        Focal Loss for Dense Object Detection.\n",
        "\n",
        "            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
        "\n",
        "        The losses are averaged across observations for each minibatch.\n",
        "\n",
        "        Args:\n",
        "            alpha(1D Tensor, Variable) : the scalar factor for this criterion\n",
        "            gamma(float, double) : gamma > 0; reduces the relative loss for well-classiﬁed examples (p > .5),\n",
        "                                   putting more focus on hard, misclassiﬁed examples\n",
        "            size_average(bool): By default, the losses are averaged over observations for each minibatch.\n",
        "                                However, if the field size_average is set to False, the losses are\n",
        "                                instead summed for each minibatch.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        if alpha is None:\n",
        "            self.alpha = Variable(torch.ones(class_num, 1))\n",
        "        else:\n",
        "            if isinstance(alpha, Variable):\n",
        "                self.alpha = alpha\n",
        "            else:\n",
        "                self.alpha = Variable(alpha)\n",
        "        self.gamma = gamma\n",
        "        self.class_num = class_num\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        N = inputs.size(0)\n",
        "        C = inputs.size(1)\n",
        "        P = F.softmax(inputs)\n",
        "\n",
        "        class_mask = inputs.data.new(N, C).fill_(0)\n",
        "        class_mask = Variable(class_mask)\n",
        "        ids = targets.view(-1, 1)\n",
        "        class_mask.scatter_(1, ids.data, 1.)\n",
        "\n",
        "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
        "            self.alpha = self.alpha.cuda()\n",
        "        alpha = self.alpha[ids.data.view(-1)]\n",
        "\n",
        "        probs = (P*class_mask).sum(1).view(-1,1)\n",
        "\n",
        "        log_p = probs.log()\n",
        "        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p\n",
        "\n",
        "        if self.size_average:\n",
        "            loss = batch_loss.mean()\n",
        "        else:\n",
        "            loss = batch_loss.sum()\n",
        "        return loss"
      ],
      "metadata": {
        "id": "mPv72RxO2u-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## \n",
        "import torch\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torch import nn\n",
        "\n",
        "class HungarianMatcher_Crowd(nn.Module):\n",
        "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
        "\n",
        "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
        "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
        "    while the others are un-matched (and thus treated as non-objects).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cost_class: float = 1, cost_point: float = 1):\n",
        "        \"\"\"Creates the matcher\n",
        "\n",
        "        Params:\n",
        "            cost_class: This is the relative weight of the foreground object\n",
        "            cost_point: This is the relative weight of the L1 error of the points coordinates in the matching cost\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_point = cost_point\n",
        "        assert cost_class != 0 or cost_point != 0, \"all costs cant be 0\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" Performs the matching\n",
        "\n",
        "        Params:\n",
        "            outputs: This is a dict that contains at least these entries:\n",
        "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
        "                 \"points\": Tensor of dim [batch_size, num_queries, 2] with the predicted point coordinates\n",
        "\n",
        "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
        "                 \"labels\": Tensor of dim [num_target_points] (where num_target_points is the number of ground-truth\n",
        "                           objects in the target) containing the class labels\n",
        "                 \"points\": Tensor of dim [num_target_points, 2] containing the target point coordinates\n",
        "\n",
        "        Returns:\n",
        "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
        "                - index_i is the indices of the selected predictions (in order)\n",
        "                - index_j is the indices of the corresponding selected targets (in order)\n",
        "            For each batch element, it holds:\n",
        "                len(index_i) = len(index_j) = min(num_queries, num_target_points)\n",
        "        \"\"\"\n",
        "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "\n",
        "        # We flatten to compute the cost matrices in a batch\n",
        "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
        "        out_points = outputs[\"pred_points\"].flatten(0, 1)  # [batch_size * num_queries, 2]\n",
        "\n",
        "        # Also concat the target labels and points\n",
        "        # tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
        "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
        "        tgt_points = torch.cat([v[\"point\"] for v in targets])\n",
        "\n",
        "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
        "        # but approximate it in 1 - proba[target class].\n",
        "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
        "        cost_class = -out_prob[:, tgt_ids]\n",
        "\n",
        "        # Compute the L2 cost between point\n",
        "        cost_point = torch.cdist(out_points, tgt_points, p=2)\n",
        "\n",
        "        # Compute the giou cost between point\n",
        "\n",
        "        # Final cost matrix\n",
        "        C = self.cost_point * cost_point + self.cost_class * cost_class\n",
        "        C = C.view(bs, num_queries, -1).cpu()\n",
        "\n",
        "        sizes = [len(v[\"point\"]) for v in targets]\n",
        "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
        "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
        "#\n",
        "def build_matcher_crowd(args):\n",
        "    return HungarianMatcher_Crowd(cost_class=args.set_cost_class, cost_point=args.set_cost_point)"
      ],
      "metadata": {
        "id": "DkJ2o7YK_1hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The feature extraction part is mainly adapted from https://github.com/zhaoyuzhi/PyTorch-Pyramid-Feature-Attention-Network-for-Saliency-Detection"
      ],
      "metadata": {
        "id": "pBf2_5jxO-D7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L669uVsdCvFy"
      },
      "outputs": [],
      "source": [
        "## \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, in_channels, kernel_size=9):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.in_channels = in_channels\n",
        "        pad = (self.kernel_size-1)//2  # Padding on one side for stride 1\n",
        "\n",
        "        self.grp1_conv1k = nn.Conv2d(self.in_channels, self.in_channels//2, (1, self.kernel_size), padding=(0, pad))\n",
        "        self.grp1_bn1 = nn.BatchNorm2d(self.in_channels//2)\n",
        "        self.grp1_convk1 = nn.Conv2d(self.in_channels//2, 1, (self.kernel_size, 1), padding=(pad, 0))\n",
        "        self.grp1_bn2 = nn.BatchNorm2d(1)\n",
        "\n",
        "        self.grp2_convk1 = nn.Conv2d(self.in_channels, self.in_channels//2, (self.kernel_size, 1), padding=(pad, 0))\n",
        "        self.grp2_bn1 = nn.BatchNorm2d(self.in_channels//2)\n",
        "        self.grp2_conv1k = nn.Conv2d(self.in_channels//2, 1, (1, self.kernel_size), padding=(0, pad))\n",
        "        self.grp2_bn2 = nn.BatchNorm2d(1)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        # Generate Group 1 Features\n",
        "        grp1_feats = self.grp1_conv1k(input_)\n",
        "        grp1_feats = F.relu(self.grp1_bn1(grp1_feats))\n",
        "        grp1_feats = self.grp1_convk1(grp1_feats)\n",
        "        grp1_feats = F.relu(self.grp1_bn2(grp1_feats))\n",
        "\n",
        "        # Generate Group 2 features\n",
        "        grp2_feats = self.grp2_convk1(input_)\n",
        "        grp2_feats = F.relu(self.grp2_bn1(grp2_feats))\n",
        "        grp2_feats = self.grp2_conv1k(grp2_feats)\n",
        "        grp2_feats = F.relu(self.grp2_bn2(grp2_feats))\n",
        "\n",
        "        added_feats = torch.sigmoid(torch.add(grp1_feats, grp2_feats))\n",
        "        added_feats = added_feats.expand_as(input_).clone()\n",
        "\n",
        "        return added_feats\n",
        "#\n",
        "class ChannelwiseAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(ChannelwiseAttention, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.linear_1 = nn.Linear(self.in_channels, self.in_channels//4)\n",
        "        self.linear_2 = nn.Linear(self.in_channels//4, self.in_channels)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        n_b, n_c, h, w = input_.size()\n",
        "\n",
        "        feats = F.adaptive_avg_pool2d(input_, (1, 1)).view((n_b, n_c))\n",
        "        feats = F.relu(self.linear_1(feats))\n",
        "        feats = torch.sigmoid(self.linear_2(feats))\n",
        "        \n",
        "        # Activity regularizer\n",
        "        ca_act_reg = torch.mean(feats)\n",
        "\n",
        "        feats = feats.view((n_b, n_c, 1, 1))\n",
        "        feats = feats.expand_as(input_).clone()\n",
        "\n",
        "        return feats, ca_act_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmPunNWUGSOe"
      },
      "outputs": [],
      "source": [
        "#### \n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "vgg_conv1_2 = vgg_conv2_2 = vgg_conv3_3 = vgg_conv4_3 = vgg_conv5_3 = None\n",
        "\n",
        "def conv_1_2_hook(module, input, output):\n",
        "    global vgg_conv1_2\n",
        "    vgg_conv1_2 = output\n",
        "    return None\n",
        "\n",
        "def conv_2_2_hook(module, input, output):\n",
        "    global vgg_conv2_2\n",
        "    vgg_conv2_2 = output\n",
        "    return None\n",
        "\n",
        "def conv_3_3_hook(module, input, output):\n",
        "    global vgg_conv3_3\n",
        "    vgg_conv3_3 = output\n",
        "    return None\n",
        "\n",
        "def conv_4_3_hook(module, input, output):\n",
        "    global vgg_conv4_3\n",
        "    vgg_conv4_3 = output\n",
        "    return None\n",
        "\n",
        "def conv_5_3_hook(module, input, output):\n",
        "    global vgg_conv5_3\n",
        "    vgg_conv5_3 = output\n",
        "    return None\n",
        "\n",
        "##\n",
        "class CPFE_hl(nn.Module):\n",
        "    def __init__(self, feature_layer=None, out_channels=8):\n",
        "        super(CPFE_hl, self).__init__()\n",
        "\n",
        "        self.dil_rates = [3, 5, 7]\n",
        "\n",
        "        # Determine number of in_channels from VGG-16 feature layer\n",
        "        if feature_layer == 'conv5_3':\n",
        "            self.in_channels = 512\n",
        "        elif feature_layer == 'conv4_3':\n",
        "            self.in_channels = 512\n",
        "        elif feature_layer == 'conv3_3':\n",
        "            self.in_channels = 256\n",
        "        elif feature_layer == 'conv2_3':\n",
        "            self.in_channels = 128\n",
        "        elif feature_layer == 'conv1_3':\n",
        "            self.in_channels = 64\n",
        "\n",
        "        # Define layers\n",
        "        self.conv_1_1 = nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=1, bias=False)\n",
        "        self.conv_dil_3 = nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=3,\n",
        "                                    stride=1, dilation=self.dil_rates[0], padding=self.dil_rates[0], bias=False)\n",
        "        self.conv_dil_5 = nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=3,\n",
        "                                    stride=1, dilation=self.dil_rates[1], padding=self.dil_rates[1], bias=False)\n",
        "        self.conv_dil_7 = nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=3,\n",
        "                                    stride=1, dilation=self.dil_rates[2], padding=self.dil_rates[2], bias=False)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels*4)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        # Extract features\n",
        "        conv_1_1_feats = self.conv_1_1(input_)\n",
        "        conv_dil_3_feats = self.conv_dil_3(input_)\n",
        "        conv_dil_5_feats = self.conv_dil_5(input_)\n",
        "        conv_dil_7_feats = self.conv_dil_7(input_)\n",
        "\n",
        "        # Aggregate features\n",
        "        concat_feats = torch.cat((conv_1_1_feats, conv_dil_3_feats, conv_dil_5_feats, conv_dil_7_feats), dim=1)\n",
        "        bn_feats = F.relu(self.bn(concat_feats))\n",
        "\n",
        "        return bn_feats\n",
        "##\n",
        "class SODModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SODModel, self).__init__()\n",
        "\n",
        "        # \n",
        "        self.vgg16 = models.vgg16(pretrained=True).features\n",
        "\n",
        "        # Extract and register intermediate features of VGG-16_bn\n",
        "        self.vgg16[3].register_forward_hook(conv_1_2_hook)\n",
        "        self.vgg16[8].register_forward_hook(conv_2_2_hook)\n",
        "        self.vgg16[15].register_forward_hook(conv_3_3_hook)\n",
        "        self.vgg16[22].register_forward_hook(conv_4_3_hook)\n",
        "        self.vgg16[29].register_forward_hook(conv_5_3_hook)\n",
        "\n",
        "        # Initialize layers for high level (hl) feature (conv3_3, conv4_3, conv5_3) processing\n",
        "        self.cpfe_conv3_3 = CPFE_hl(feature_layer='conv3_3')\n",
        "        self.cpfe_conv4_3 = CPFE_hl(feature_layer='conv4_3')\n",
        "        self.cpfe_conv5_3 = CPFE_hl(feature_layer='conv5_3')\n",
        "        #\n",
        "        self.cpfe_conv1_3 = CPFE_hl(feature_layer='conv1_3')\n",
        "        self.cpfe_conv2_3 = CPFE_hl(feature_layer='conv2_3')\n",
        "        # 11,03,2022, remove channel attention\n",
        "        self.cha_att = ChannelwiseAttention(in_channels=96)  # in_channels = 3 x (8 x 4)\n",
        "\n",
        "        self.hl_conv1 = nn.Conv2d(96, 8, (3, 3), padding=1)\n",
        "        self.hl_bn1 = nn.BatchNorm2d(8)\n",
        "\n",
        "        # \n",
        "        self.ll_conv_1 = nn.Conv2d(64, 8, (3, 3), padding=1)\n",
        "        self.ll_bn_1 = nn.BatchNorm2d(8)\n",
        "        self.ll_conv_2 = nn.Conv2d(128, 8, (3, 3), padding=1)\n",
        "        self.ll_bn_2 = nn.BatchNorm2d(8)\n",
        "        self.ll_conv_3 = nn.Conv2d(64, 8, (3, 3), padding=1) \n",
        "        self.ll_bn_3 = nn.BatchNorm2d(8)\n",
        "\n",
        "        self.spa_att = SpatialAttention(in_channels=8)\n",
        "\n",
        "        # \n",
        "        self.ff_conv_1 = nn.Conv2d(16, 3, (3, 3), padding=1)\n",
        "        self.ff_bn_1 = nn.BatchNorm2d(3)\n",
        "    def forward(self, input_):\n",
        "        global vgg_conv1_2, vgg_conv2_2, vgg_conv3_3, vgg_conv4_3, vgg_conv5_3\n",
        "\n",
        "        # Pass input_ through vgg16 to generate intermediate features\n",
        "        self.vgg16(input_)\n",
        "        # Process high level features\n",
        "        conv3_cpfe_feats = self.cpfe_conv3_3(vgg_conv3_3)\n",
        "        conv4_cpfe_feats = self.cpfe_conv4_3(vgg_conv4_3)\n",
        "        conv5_cpfe_feats = self.cpfe_conv5_3(vgg_conv5_3)\n",
        "\n",
        "        conv4_cpfe_feats = F.interpolate(conv4_cpfe_feats, scale_factor=2, mode='bilinear', align_corners=True) # reduce spatial dimension by 2\n",
        "        conv5_cpfe_feats = F.interpolate(conv5_cpfe_feats, scale_factor=4, mode='bilinear', align_corners=True)\n",
        "\n",
        "        conv_345_feats = torch.cat((conv3_cpfe_feats, conv4_cpfe_feats, conv5_cpfe_feats), dim=1)\n",
        "        \n",
        "        # channel attention on high level features\n",
        "        conv_345_ca, ca_act_reg = self.cha_att(conv_345_feats)\n",
        "        conv_345_feats = torch.mul(conv_345_feats, conv_345_ca)\n",
        "\n",
        "        conv_345_feats = self.hl_conv1(conv_345_feats)\n",
        "        conv_345_feats = F.relu(self.hl_bn1(conv_345_feats))\n",
        "        ##\n",
        "        # Process low level features\n",
        "        conv0_feats = input_ # the original input image\n",
        "        conv1_cpfe_feats = self.cpfe_conv1_3(vgg_conv1_2)\n",
        "        conv2_cpfe_feats = self.cpfe_conv2_3(vgg_conv2_2)\n",
        "\n",
        "        conv0_feats = F.interpolate(conv0_feats, scale_factor=0.25, mode='bilinear', align_corners=True)\n",
        "        conv1_cpfe_feats = F.interpolate(conv1_cpfe_feats, scale_factor=0.25, mode='bilinear', align_corners=True)\n",
        "        conv2_cpfe_feats = F.interpolate(conv2_cpfe_feats, scale_factor=0.5, mode='bilinear', align_corners=True)\n",
        "\n",
        "        #\n",
        "        conv_12_feats = torch.cat((conv1_cpfe_feats, conv2_cpfe_feats), dim=1)\n",
        "        conv_12_feats = self.ll_conv_3(conv_12_feats)\n",
        "        conv_12_feats = F.relu(self.ll_bn_3(conv_12_feats))\n",
        "        # spatial attention on low level features\n",
        "        conv_12_sa = self.spa_att(conv_12_feats)\n",
        "        conv_12_feats = torch.mul(conv_12_feats, conv_12_sa)\n",
        "\n",
        "        # fuse the low and high level features\n",
        "        fused_feats = torch.cat((conv_12_feats, conv_345_feats), dim=1)\n",
        "        #\n",
        "        fused_final = self.ff_conv_1(fused_feats)\n",
        "        fused_final = F.relu(self.ff_bn_1(fused_final))\n",
        "        # add the fused low and high level features to the original image\n",
        "        fused_final_out = torch.add(fused_final, conv0_feats)\n",
        "        fused_final_out = torch.sigmoid(fused_final_out)\n",
        "\n",
        "        return fused_final_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNz8iOaxdj6d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3M_OTlkVPO5"
      },
      "outputs": [],
      "source": [
        "# build model p2pNet.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# the network frmawork of the regression branch\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, num_features_in, num_anchor_points=4, feature_size=32):\n",
        "        super(RegressionModel, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "        self.act3 = nn.ReLU()\n",
        "\n",
        "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "        self.act4 = nn.ReLU()\n",
        "\n",
        "        self.output = nn.Conv2d(feature_size, num_anchor_points * 2, kernel_size=3, padding=1) # one point has two coordinates \n",
        "    # sub-branch forward\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.act1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.act2(out)\n",
        "\n",
        "        out = self.output(out)\n",
        "\n",
        "        out = out.permute(0, 2, 3, 1)\n",
        "\n",
        "        return out.contiguous().view(out.shape[0], -1, 2)\n",
        "\n",
        "# the network frmawork of the classification branch\n",
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self, num_features_in, num_anchor_points=4, num_classes=80, prior=0.01, feature_size=32):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_anchor_points = num_anchor_points\n",
        "\n",
        "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "        self.act3 = nn.ReLU()\n",
        "\n",
        "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "        self.act4 = nn.ReLU()\n",
        "\n",
        "        self.output = nn.Conv2d(feature_size, num_anchor_points * num_classes, kernel_size=3, padding=1) # one classes, only positives \n",
        "        self.output_act = nn.Sigmoid()\n",
        "    # sub-branch forward\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.act1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.act2(out)\n",
        "\n",
        "        out = self.output(out)\n",
        "\n",
        "        out1 = out.permute(0, 2, 3, 1)\n",
        "\n",
        "        batch_size, width, height, _ = out1.shape\n",
        "\n",
        "        out2 = out1.view(batch_size, width, height, self.num_anchor_points, self.num_classes)\n",
        "\n",
        "        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n",
        "\n",
        "# generate the reference points in grid layout\n",
        "def generate_anchor_points(stride=8, row=3, line=3):\n",
        "    row_step = stride / row\n",
        "    line_step = stride / line\n",
        "\n",
        "    shift_x = (np.arange(1, line + 1) - 0.5) * line_step - stride / 2\n",
        "    shift_y = (np.arange(1, row + 1) - 0.5) * row_step - stride / 2\n",
        "\n",
        "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
        "\n",
        "    anchor_points = np.vstack((\n",
        "        shift_x.ravel(), shift_y.ravel()\n",
        "    )).transpose()\n",
        "\n",
        "    return anchor_points\n",
        "# shift the meta-anchor to get an acnhor points\n",
        "def shift(shape, stride, anchor_points):\n",
        "    shift_x = (np.arange(0, shape[1]) + 0.5)* stride\n",
        "    shift_y = (np.arange(0, shape[0]) + 0.5)* stride\n",
        "\n",
        "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
        "\n",
        "    shifts = np.vstack((\n",
        "        shift_x.ravel(), shift_y.ravel()\n",
        "    )).transpose()\n",
        "\n",
        "    A = anchor_points.shape[0]\n",
        "    K = shifts.shape[0]\n",
        "    all_anchor_points = (anchor_points.reshape((1, A, 2)) + shifts.reshape((1, K, 2)).transpose((1, 0, 2)))\n",
        "    all_anchor_points = all_anchor_points.reshape((K * A, 2))\n",
        "\n",
        "    return all_anchor_points\n",
        "\n",
        "# \n",
        "class AnchorPoints(nn.Module):\n",
        "    def __init__(self, pyramid_levels=None, strides=None, row=3, line=3):\n",
        "        super(AnchorPoints, self).__init__()\n",
        "\n",
        "        if pyramid_levels is None:\n",
        "            self.pyramid_levels = [3, 4, 5, 6, 7]\n",
        "        else:\n",
        "            self.pyramid_levels = pyramid_levels\n",
        "\n",
        "        if strides is None:\n",
        "            self.strides = [2 ** x for x in self.pyramid_levels]\n",
        "\n",
        "        self.row = row\n",
        "        self.line = line\n",
        "\n",
        "    def forward(self, image):\n",
        "        image_shape = image.shape[2:]\n",
        "        image_shape = np.array(image_shape)\n",
        "        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels] # calcualtes the output size of the model (image of 128*128 to feature map of 16*16)\n",
        "\n",
        "        all_anchor_points = np.zeros((0, 2)).astype(np.float32)\n",
        "        # get reference points for each level\n",
        "        for idx, p in enumerate(self.pyramid_levels):\n",
        "            anchor_points = generate_anchor_points(2**p, row=self.row, line=self.line)\n",
        "            shifted_anchor_points = shift(image_shapes[idx], self.strides[idx], anchor_points)\n",
        "            all_anchor_points = np.append(all_anchor_points, shifted_anchor_points, axis=0)\n",
        "\n",
        "        all_anchor_points = np.expand_dims(all_anchor_points, axis=0)\n",
        "        # send reference points to device\n",
        "        if torch.cuda.is_available():\n",
        "            return torch.from_numpy(all_anchor_points.astype(np.float32)).cuda()\n",
        "        else:\n",
        "            return torch.from_numpy(all_anchor_points.astype(np.float32))\n",
        "##\n",
        "# the defenition of the P2PNet model\n",
        "class P2PNet(nn.Module):\n",
        "    def __init__(self, row=2, line=2):\n",
        "        super().__init__()\n",
        "        self.num_classes = 2\n",
        "        # the number of all anchor points\n",
        "        num_anchor_points = row * line\n",
        "\n",
        "        self.regression = RegressionModel(num_features_in=3, num_anchor_points=num_anchor_points)\n",
        "        self.classification = ClassificationModel(num_features_in=3, \\\n",
        "                                            num_classes=self.num_classes, \\\n",
        "                                            num_anchor_points=num_anchor_points)\n",
        "\n",
        "        self.anchor_points = AnchorPoints(pyramid_levels=[2,], row=row, line=line) # remember to change pyramid level when you change feature input\n",
        "\n",
        "        self.fpn = SODModel()\n",
        "\n",
        "    def forward(self, samples: NestedTensor): #\n",
        "        # \n",
        "        features_fpn = self.fpn(samples) # output = bach_size, channel, Height, Weight\n",
        "        #\n",
        "        batch_size = features_fpn.size()[0]\n",
        "        # run the regression and classification branch\n",
        "        regression = self.regression(features_fpn) * 100 # 8x\n",
        "        classification = self.classification(features_fpn)\n",
        "        #\n",
        "        anchor_points = self.anchor_points(samples).repeat(batch_size, 1, 1)\n",
        "        output_coord = regression + anchor_points\n",
        "        output_class = classification\n",
        "        out = {'pred_logits': output_class, 'pred_points': output_coord}\n",
        "        #\n",
        "        return out\n",
        "\n",
        "class SetCriterion_Crowd(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
        "        \"\"\" Create the criterion.\n",
        "        Parameters:\n",
        "            num_classes: number of object categories, omitting the special no-object category\n",
        "            matcher: module able to compute a matching between targets and proposals\n",
        "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
        "            eos_coef: relative classification weight applied to the no-object category\n",
        "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.eos_coef = eos_coef\n",
        "        self.losses = losses\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[0] = self.eos_coef\n",
        "        self.register_buffer('empty_weight', empty_weight)\n",
        "\n",
        "    def loss_labels(self, outputs, targets, indices, num_points):\n",
        "        \"\"\"Classification loss (NLL)\n",
        "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
        "        \"\"\"\n",
        "        assert 'pred_logits' in outputs\n",
        "        src_logits = outputs['pred_logits']\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
        "        target_classes = torch.full(src_logits.shape[:2], 0,\n",
        "                                    dtype=torch.int64, device=src_logits.device)\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
        "        losses = {'loss_ce': loss_ce}\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def loss_points(self, outputs, targets, indices, num_points):\n",
        "\n",
        "        assert 'pred_points' in outputs\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_points = outputs['pred_points'][idx]\n",
        "        target_points = torch.cat([t['point'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "        #print(\"target_points {}\".format(target_points))\n",
        "        loss_bbox = F.mse_loss(src_points, target_points, reduction='none')\n",
        "\n",
        "        losses = {}\n",
        "        losses['loss_points'] = loss_bbox.sum() / num_points\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, indices, num_points, **kwargs):\n",
        "        loss_map = {\n",
        "            'labels': self.loss_labels,\n",
        "            'points': self.loss_points,\n",
        "        }\n",
        "        #print(\"loss_map {}\".format(loss_map))\n",
        "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
        "        return loss_map[loss](outputs, targets, indices, num_points, **kwargs)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" This performs the loss computation.\n",
        "        Parameters:\n",
        "             outputs: dict of tensors, see the output specification of the model for the format\n",
        "             targets: list of dicts, such that len(targets) == batch_size.\n",
        "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
        "        \"\"\"\n",
        "        output1 = {'pred_logits': outputs['pred_logits'], 'pred_points': outputs['pred_points']}\n",
        "\n",
        "        indices1 = self.matcher(output1, targets)\n",
        "\n",
        "        num_points = sum(len(t[\"labels\"]) for t in targets)\n",
        "\n",
        "        num_points = torch.as_tensor([num_points], dtype=torch.float, device=next(iter(output1.values())).device)\n",
        "\n",
        "        num_boxes = torch.clamp(num_points / get_world_size(), min=1).item()\n",
        "\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self.get_loss(loss, output1, targets, indices1, num_boxes)) #\n",
        "\n",
        "        return losses\n",
        "\n",
        "# create the P2PNet model\n",
        "def build(args, training):\n",
        "    # treats persons as a single class\n",
        "    num_classes = 1\n",
        "\n",
        "    model = P2PNet(args.row, args.line)\n",
        "    if not training: \n",
        "        return model\n",
        "\n",
        "    weight_dict = {'loss_ce': 1, 'loss_points': args.point_loss_coef}\n",
        "    losses = ['labels', 'points']     #['labels', 'points']\n",
        "    matcher = build_matcher_crowd(args)\n",
        "    criterion = SetCriterion_Crowd(num_classes, \\\n",
        "                                matcher=matcher, weight_dict=weight_dict, \\\n",
        "                                eos_coef=args.eos_coef, losses=losses)\n",
        "\n",
        "    return model, criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List all the parameters here"
      ],
      "metadata": {
        "id": "O3Gd0Z3JSDE8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zllbyl5BOlQj"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_arguments():\n",
        "    \"\"\"Parse all the arguments provided from the CLI.\n",
        "    Returns:\n",
        "      A list of parsed arguments.\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"Object Counting Framework\")\n",
        "    # constant\n",
        "    parser.add_argument('--lr', default=1e-4, type=float)\n",
        "    parser.add_argument('--lr_fpn', default=1e-5, type=float)\n",
        "    parser.add_argument('--batch_size', default=1, type=int)\n",
        "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
        "    parser.add_argument('--epochs', default=3500, type=int)\n",
        "    parser.add_argument('--lr_drop', default=100, type=int)\n",
        "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
        "                        help='gradient clipping max norm')\n",
        "\n",
        "    # Model parameters\n",
        "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
        "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
        "    #\n",
        "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
        "                        help=\"Class coefficient in the matching cost\")\n",
        "\n",
        "    parser.add_argument('--set_cost_point', default=0.99, type=float,\n",
        "                        help=\"L1 point coefficient in the matching cost\")\n",
        "\n",
        "    # * Loss coefficients\n",
        "    parser.add_argument('--point_loss_coef', default=0.02, type=float) # default = 0.0002 # 0.5\n",
        "    parser.add_argument('--eos_coef', default=0.02, type=float, # 0.05\n",
        "                        help=\"Relative classification weight of the no-object class\") # default = 0.5\n",
        "    \n",
        "    # a threshold during evaluation for counting and visualization\n",
        "    parser.add_argument('--threshold', default=0.5, type=float,\n",
        "                        help=\"threshold in evalluation: evaluate_crowd_no_overlap\")\n",
        "    parser.add_argument('--row', default=2, type=int,\n",
        "                        help=\"row number of anchor points\")\n",
        "    parser.add_argument('--line', default=2, type=int,\n",
        "                        help=\"line number of anchor points\")\n",
        "\n",
        "    # dataset parameters\n",
        "    parser.add_argument('--dataset_file', default='SHHA')\n",
        "    parser.add_argument('--data_root', default='/content/drive/My Drive/P2PNet-Soy/Soybean_seed_counting/',\n",
        "                        help='path where the dataset is')\n",
        "    \n",
        "    parser.add_argument('--output_dir', default='/content/drive/My Drive/P2PNet-Soy/log_p2pnetSoy_testing',\n",
        "                        help='path where to save, empty for no saving')\n",
        "    parser.add_argument('--checkpoints_dir', default='/content/drive/My Drive/P2PNet-Soy/ckpt_p2pnetSoy_testing',\n",
        "                        help='path where to save checkpoints, empty for no saving') \n",
        "    parser.add_argument('--tensorboard_dir', default='/content/drive/My Drive/P2PNet-Soy/runs_p2pnetSoy_testing',\n",
        "                        help='path where to save, empty for no saving')\n",
        "\n",
        "    parser.add_argument('--seed', default=42, type=int)\n",
        "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
        "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
        "                        help='start epoch')\n",
        "    parser.add_argument('--eval', action='store_true')\n",
        "    parser.add_argument('--num_workers', default=1, type=int)\n",
        "    parser.add_argument('--eval_freq', default=3, type=int,\n",
        "                        help='frequency of evaluation, default setting is evaluating in every 5 epoch')\n",
        "    parser.add_argument('--gpu_id', default=0, type=int, help='the gpu used for training')\n",
        "    #\n",
        "    opt = parser.parse_known_args()[0]\n",
        "    return opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl6zkwnxkqJs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzU3pfq1Js4r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a9dc534-31e6-4468-a08e-1148adfcafbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluation time 315.9228549003601\n",
            "=======================================test=======================================\n",
            "mae: 18.28787878787879 mse: 24.87179246869957 time: 315.9228549003601 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0412 (0.0545)  loss_ce: 0.0306 (0.0430)  loss_points: 0.0116 (0.0115)  loss_ce_unscaled: 0.0306 (0.0430)  loss_points_unscaled: 0.5803 (0.5758)\n",
            "[ep 301][lr 0.0000010][65.28s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0401 (0.0520)  loss_ce: 0.0289 (0.0404)  loss_points: 0.0114 (0.0116)  loss_ce_unscaled: 0.0289 (0.0404)  loss_points_unscaled: 0.5678 (0.5809)\n",
            "[ep 302][lr 0.0000010][64.22s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0385 (0.0546)  loss_ce: 0.0268 (0.0428)  loss_points: 0.0113 (0.0117)  loss_ce_unscaled: 0.0268 (0.0428)  loss_points_unscaled: 0.5627 (0.5867)\n",
            "[ep 303][lr 0.0000010][64.63s]\n",
            "evaluation time 300.5980007648468\n",
            "=======================================test=======================================\n",
            "mae: 18.015151515151516 mse: 23.14414645545424 time: 300.5980007648468 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0409 (0.0538)  loss_ce: 0.0303 (0.0421)  loss_points: 0.0110 (0.0116)  loss_ce_unscaled: 0.0303 (0.0421)  loss_points_unscaled: 0.5510 (0.5821)\n",
            "[ep 304][lr 0.0000010][65.59s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0438 (0.0520)  loss_ce: 0.0321 (0.0404)  loss_points: 0.0114 (0.0116)  loss_ce_unscaled: 0.0321 (0.0404)  loss_points_unscaled: 0.5717 (0.5816)\n",
            "[ep 305][lr 0.0000010][65.31s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0378 (0.0562)  loss_ce: 0.0272 (0.0445)  loss_points: 0.0116 (0.0117)  loss_ce_unscaled: 0.0272 (0.0445)  loss_points_unscaled: 0.5820 (0.5833)\n",
            "[ep 306][lr 0.0000010][66.25s]\n",
            "evaluation time 317.0581429004669\n",
            "=======================================test=======================================\n",
            "mae: 18.40909090909091 mse: 25.413251136628176 time: 317.0581429004669 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0554 (0.0599)  loss_ce: 0.0439 (0.0483)  loss_points: 0.0115 (0.0115)  loss_ce_unscaled: 0.0439 (0.0483)  loss_points_unscaled: 0.5755 (0.5767)\n",
            "[ep 307][lr 0.0000010][65.77s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0575 (0.0575)  loss_ce: 0.0456 (0.0459)  loss_points: 0.0111 (0.0116)  loss_ce_unscaled: 0.0456 (0.0459)  loss_points_unscaled: 0.5540 (0.5789)\n",
            "[ep 308][lr 0.0000010][65.62s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0474 (0.0532)  loss_ce: 0.0361 (0.0415)  loss_points: 0.0115 (0.0117)  loss_ce_unscaled: 0.0361 (0.0415)  loss_points_unscaled: 0.5739 (0.5851)\n",
            "[ep 309][lr 0.0000010][65.10s]\n",
            "evaluation time 323.7006289958954\n",
            "=======================================test=======================================\n",
            "mae: 17.318181818181817 mse: 24.459923932585703 time: 323.7006289958954 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0434 (0.0533)  loss_ce: 0.0325 (0.0417)  loss_points: 0.0113 (0.0116)  loss_ce_unscaled: 0.0325 (0.0417)  loss_points_unscaled: 0.5650 (0.5801)\n",
            "[ep 310][lr 0.0000010][64.39s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0412 (0.0547)  loss_ce: 0.0295 (0.0432)  loss_points: 0.0119 (0.0115)  loss_ce_unscaled: 0.0295 (0.0432)  loss_points_unscaled: 0.5933 (0.5760)\n",
            "[ep 311][lr 0.0000010][63.61s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0418 (0.0539)  loss_ce: 0.0291 (0.0424)  loss_points: 0.0117 (0.0115)  loss_ce_unscaled: 0.0291 (0.0424)  loss_points_unscaled: 0.5845 (0.5754)\n",
            "[ep 312][lr 0.0000010][64.49s]\n",
            "evaluation time 296.89069843292236\n",
            "=======================================test=======================================\n",
            "mae: 17.575757575757574 mse: 22.561699914463514 time: 296.89069843292236 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0433 (0.0530)  loss_ce: 0.0323 (0.0413)  loss_points: 0.0114 (0.0117)  loss_ce_unscaled: 0.0323 (0.0413)  loss_points_unscaled: 0.5718 (0.5855)\n",
            "[ep 313][lr 0.0000010][63.79s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0399 (0.0518)  loss_ce: 0.0275 (0.0401)  loss_points: 0.0111 (0.0117)  loss_ce_unscaled: 0.0275 (0.0401)  loss_points_unscaled: 0.5546 (0.5844)\n",
            "[ep 314][lr 0.0000010][63.69s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0435 (0.0539)  loss_ce: 0.0327 (0.0425)  loss_points: 0.0111 (0.0114)  loss_ce_unscaled: 0.0327 (0.0425)  loss_points_unscaled: 0.5532 (0.5717)\n",
            "[ep 315][lr 0.0000010][64.63s]\n",
            "evaluation time 304.28403425216675\n",
            "=======================================test=======================================\n",
            "mae: 17.09090909090909 mse: 22.776915480105703 time: 304.28403425216675 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0384 (0.0522)  loss_ce: 0.0269 (0.0406)  loss_points: 0.0116 (0.0116)  loss_ce_unscaled: 0.0269 (0.0406)  loss_points_unscaled: 0.5805 (0.5803)\n",
            "[ep 316][lr 0.0000010][64.19s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0438 (0.0516)  loss_ce: 0.0328 (0.0401)  loss_points: 0.0115 (0.0115)  loss_ce_unscaled: 0.0328 (0.0401)  loss_points_unscaled: 0.5739 (0.5753)\n",
            "[ep 317][lr 0.0000010][63.98s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0378 (0.0524)  loss_ce: 0.0257 (0.0408)  loss_points: 0.0114 (0.0116)  loss_ce_unscaled: 0.0257 (0.0408)  loss_points_unscaled: 0.5689 (0.5810)\n",
            "[ep 318][lr 0.0000010][65.19s]\n",
            "evaluation time 321.0874843597412\n",
            "=======================================test=======================================\n",
            "mae: 18.174242424242426 mse: 25.30076654725678 time: 321.0874843597412 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0389 (0.0514)  loss_ce: 0.0275 (0.0399)  loss_points: 0.0112 (0.0115)  loss_ce_unscaled: 0.0275 (0.0399)  loss_points_unscaled: 0.5609 (0.5737)\n",
            "[ep 319][lr 0.0000010][63.91s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0414 (0.0534)  loss_ce: 0.0302 (0.0419)  loss_points: 0.0110 (0.0115)  loss_ce_unscaled: 0.0302 (0.0419)  loss_points_unscaled: 0.5524 (0.5767)\n",
            "[ep 320][lr 0.0000010][64.30s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0493 (0.0512)  loss_ce: 0.0382 (0.0397)  loss_points: 0.0114 (0.0115)  loss_ce_unscaled: 0.0382 (0.0397)  loss_points_unscaled: 0.5693 (0.5754)\n",
            "[ep 321][lr 0.0000010][63.85s]\n",
            "evaluation time 320.9733874797821\n",
            "=======================================test=======================================\n",
            "mae: 19.234848484848484 mse: 26.653358421623253 time: 320.9733874797821 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0394 (0.0554)  loss_ce: 0.0278 (0.0437)  loss_points: 0.0112 (0.0116)  loss_ce_unscaled: 0.0278 (0.0437)  loss_points_unscaled: 0.5581 (0.5824)\n",
            "[ep 322][lr 0.0000010][64.90s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0452 (0.0590)  loss_ce: 0.0341 (0.0474)  loss_points: 0.0114 (0.0116)  loss_ce_unscaled: 0.0341 (0.0474)  loss_points_unscaled: 0.5681 (0.5825)\n",
            "[ep 323][lr 0.0000010][64.86s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0415 (0.0583)  loss_ce: 0.0302 (0.0467)  loss_points: 0.0114 (0.0116)  loss_ce_unscaled: 0.0302 (0.0467)  loss_points_unscaled: 0.5708 (0.5791)\n",
            "[ep 324][lr 0.0000010][65.28s]\n",
            "evaluation time 300.4147102832794\n",
            "=======================================test=======================================\n",
            "mae: 17.772727272727273 mse: 22.58686934186911 time: 300.4147102832794 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0410 (0.0561)  loss_ce: 0.0293 (0.0444)  loss_points: 0.0113 (0.0116)  loss_ce_unscaled: 0.0293 (0.0444)  loss_points_unscaled: 0.5672 (0.5817)\n",
            "[ep 325][lr 0.0000010][64.49s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0437 (0.0533)  loss_ce: 0.0326 (0.0418)  loss_points: 0.0111 (0.0115)  loss_ce_unscaled: 0.0326 (0.0418)  loss_points_unscaled: 0.5542 (0.5752)\n",
            "[ep 326][lr 0.0000010][64.80s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0458 (0.0510)  loss_ce: 0.0335 (0.0394)  loss_points: 0.0118 (0.0117)  loss_ce_unscaled: 0.0335 (0.0394)  loss_points_unscaled: 0.5900 (0.5827)\n",
            "[ep 327][lr 0.0000010][63.59s]\n",
            "evaluation time 305.80476808547974\n",
            "=======================================test=======================================\n",
            "mae: 17.424242424242426 mse: 23.102866147784006 time: 305.80476808547974 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0402 (0.0560)  loss_ce: 0.0288 (0.0444)  loss_points: 0.0118 (0.0116)  loss_ce_unscaled: 0.0288 (0.0444)  loss_points_unscaled: 0.5893 (0.5788)\n",
            "[ep 328][lr 0.0000010][64.80s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0424 (0.0584)  loss_ce: 0.0311 (0.0468)  loss_points: 0.0115 (0.0116)  loss_ce_unscaled: 0.0311 (0.0468)  loss_points_unscaled: 0.5749 (0.5789)\n",
            "[ep 329][lr 0.0000010][64.48s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0470 (0.0516)  loss_ce: 0.0357 (0.0400)  loss_points: 0.0113 (0.0116)  loss_ce_unscaled: 0.0357 (0.0400)  loss_points_unscaled: 0.5629 (0.5801)\n",
            "[ep 330][lr 0.0000010][63.75s]\n",
            "evaluation time 313.184219121933\n",
            "=======================================test=======================================\n",
            "mae: 17.40909090909091 mse: 23.67360120343743 time: 313.184219121933 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0377 (0.0534)  loss_ce: 0.0275 (0.0416)  loss_points: 0.0114 (0.0118)  loss_ce_unscaled: 0.0275 (0.0416)  loss_points_unscaled: 0.5684 (0.5877)\n",
            "[ep 331][lr 0.0000010][63.45s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0466 (0.0560)  loss_ce: 0.0341 (0.0444)  loss_points: 0.0122 (0.0116)  loss_ce_unscaled: 0.0341 (0.0444)  loss_points_unscaled: 0.6089 (0.5807)\n",
            "[ep 332][lr 0.0000010][64.70s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0442 (0.0532)  loss_ce: 0.0332 (0.0415)  loss_points: 0.0113 (0.0116)  loss_ce_unscaled: 0.0332 (0.0415)  loss_points_unscaled: 0.5657 (0.5821)\n",
            "[ep 333][lr 0.0000010][64.82s]\n",
            "evaluation time 303.3732662200928\n",
            "=======================================test=======================================\n",
            "mae: 17.484848484848484 mse: 22.8025118233616 time: 303.3732662200928 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0411 (0.0545)  loss_ce: 0.0305 (0.0429)  loss_points: 0.0118 (0.0116)  loss_ce_unscaled: 0.0305 (0.0429)  loss_points_unscaled: 0.5920 (0.5809)\n",
            "[ep 334][lr 0.0000010][64.12s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0456 (0.0522)  loss_ce: 0.0356 (0.0404)  loss_points: 0.0115 (0.0118)  loss_ce_unscaled: 0.0356 (0.0404)  loss_points_unscaled: 0.5736 (0.5887)\n",
            "[ep 335][lr 0.0000010][63.64s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0425 (0.0531)  loss_ce: 0.0304 (0.0414)  loss_points: 0.0117 (0.0117)  loss_ce_unscaled: 0.0304 (0.0414)  loss_points_unscaled: 0.5860 (0.5836)\n",
            "[ep 336][lr 0.0000010][64.02s]\n",
            "evaluation time 301.0284011363983\n",
            "=======================================test=======================================\n",
            "mae: 17.522727272727273 mse: 22.802345706022653 time: 301.0284011363983 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0428 (0.0558)  loss_ce: 0.0317 (0.0442)  loss_points: 0.0116 (0.0116)  loss_ce_unscaled: 0.0317 (0.0442)  loss_points_unscaled: 0.5783 (0.5812)\n",
            "[ep 337][lr 0.0000010][64.45s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0471 (0.0539)  loss_ce: 0.0357 (0.0422)  loss_points: 0.0112 (0.0117)  loss_ce_unscaled: 0.0357 (0.0422)  loss_points_unscaled: 0.5620 (0.5832)\n",
            "[ep 338][lr 0.0000010][63.97s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0477 (0.0548)  loss_ce: 0.0347 (0.0431)  loss_points: 0.0116 (0.0117)  loss_ce_unscaled: 0.0347 (0.0431)  loss_points_unscaled: 0.5824 (0.5869)\n",
            "[ep 339][lr 0.0000010][64.61s]\n",
            "evaluation time 347.8041133880615\n",
            "=======================================test=======================================\n",
            "mae: 21.742424242424242 mse: 30.511796029891407 time: 347.8041133880615 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0343 (0.0508)  loss_ce: 0.0224 (0.0394)  loss_points: 0.0113 (0.0114)  loss_ce_unscaled: 0.0224 (0.0394)  loss_points_unscaled: 0.5673 (0.5694)\n",
            "[ep 340][lr 0.0000010][63.45s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0430 (0.0532)  loss_ce: 0.0312 (0.0417)  loss_points: 0.0117 (0.0116)  loss_ce_unscaled: 0.0312 (0.0417)  loss_points_unscaled: 0.5846 (0.5783)\n",
            "[ep 341][lr 0.0000010][64.26s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0445 (0.0512)  loss_ce: 0.0327 (0.0397)  loss_points: 0.0115 (0.0116)  loss_ce_unscaled: 0.0327 (0.0397)  loss_points_unscaled: 0.5760 (0.5778)\n",
            "[ep 342][lr 0.0000010][63.43s]\n",
            "evaluation time 334.7918543815613\n",
            "=======================================test=======================================\n",
            "mae: 19.0 mse: 26.75477867256954 time: 334.7918543815613 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0440 (0.0583)  loss_ce: 0.0341 (0.0468)  loss_points: 0.0113 (0.0116)  loss_ce_unscaled: 0.0341 (0.0468)  loss_points_unscaled: 0.5657 (0.5791)\n",
            "[ep 343][lr 0.0000010][66.16s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0434 (0.0512)  loss_ce: 0.0320 (0.0396)  loss_points: 0.0114 (0.0116)  loss_ce_unscaled: 0.0320 (0.0396)  loss_points_unscaled: 0.5697 (0.5806)\n",
            "[ep 344][lr 0.0000010][63.12s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0393 (0.0569)  loss_ce: 0.0277 (0.0452)  loss_points: 0.0112 (0.0117)  loss_ce_unscaled: 0.0277 (0.0452)  loss_points_unscaled: 0.5596 (0.5838)\n",
            "[ep 345][lr 0.0000010][65.52s]\n",
            "evaluation time 321.1043224334717\n",
            "=======================================test=======================================\n",
            "mae: 17.75 mse: 23.709574846334537 time: 321.1043224334717 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0388 (0.0544)  loss_ce: 0.0268 (0.0429)  loss_points: 0.0116 (0.0116)  loss_ce_unscaled: 0.0268 (0.0429)  loss_points_unscaled: 0.5783 (0.5795)\n",
            "[ep 346][lr 0.0000010][64.14s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0490 (0.0510)  loss_ce: 0.0373 (0.0392)  loss_points: 0.0119 (0.0118)  loss_ce_unscaled: 0.0373 (0.0392)  loss_points_unscaled: 0.5944 (0.5886)\n",
            "[ep 347][lr 0.0000010][63.08s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0457 (0.0538)  loss_ce: 0.0329 (0.0421)  loss_points: 0.0116 (0.0117)  loss_ce_unscaled: 0.0329 (0.0421)  loss_points_unscaled: 0.5778 (0.5852)\n",
            "[ep 348][lr 0.0000010][64.56s]\n",
            "evaluation time 320.9902868270874\n",
            "=======================================test=======================================\n",
            "mae: 17.939393939393938 mse: 25.027257867445975 time: 320.9902868270874 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0414 (0.0443)  loss_ce: 0.0293 (0.0327)  loss_points: 0.0119 (0.0116)  loss_ce_unscaled: 0.0293 (0.0327)  loss_points_unscaled: 0.5933 (0.5819)\n",
            "[ep 349][lr 0.0000010][62.19s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0407 (0.0532)  loss_ce: 0.0302 (0.0416)  loss_points: 0.0113 (0.0115)  loss_ce_unscaled: 0.0302 (0.0416)  loss_points_unscaled: 0.5668 (0.5768)\n",
            "[ep 350][lr 0.0000010][64.00s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0386 (0.0575)  loss_ce: 0.0264 (0.0459)  loss_points: 0.0116 (0.0115)  loss_ce_unscaled: 0.0264 (0.0459)  loss_points_unscaled: 0.5810 (0.5768)\n",
            "[ep 351][lr 0.0000010][64.61s]\n",
            "evaluation time 320.8955364227295\n",
            "=======================================test=======================================\n",
            "mae: 17.886363636363637 mse: 24.852443328452424 time: 320.8955364227295 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0477 (0.0544)  loss_ce: 0.0377 (0.0428)  loss_points: 0.0113 (0.0115)  loss_ce_unscaled: 0.0377 (0.0428)  loss_points_unscaled: 0.5655 (0.5768)\n",
            "[ep 352][lr 0.0000010][63.98s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0399 (0.0527)  loss_ce: 0.0275 (0.0411)  loss_points: 0.0114 (0.0116)  loss_ce_unscaled: 0.0275 (0.0411)  loss_points_unscaled: 0.5721 (0.5806)\n",
            "[ep 353][lr 0.0000010][64.35s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0388 (0.0533)  loss_ce: 0.0292 (0.0417)  loss_points: 0.0117 (0.0116)  loss_ce_unscaled: 0.0292 (0.0417)  loss_points_unscaled: 0.5836 (0.5824)\n",
            "[ep 354][lr 0.0000010][64.13s]\n",
            "evaluation time 309.1140751838684\n",
            "=======================================test=======================================\n",
            "mae: 17.401515151515152 mse: 23.221757039466244 time: 309.1140751838684 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0465 (0.0531)  loss_ce: 0.0360 (0.0415)  loss_points: 0.0109 (0.0116)  loss_ce_unscaled: 0.0360 (0.0415)  loss_points_unscaled: 0.5456 (0.5797)\n",
            "[ep 355][lr 0.0000010][63.91s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0452 (0.0498)  loss_ce: 0.0333 (0.0383)  loss_points: 0.0111 (0.0115)  loss_ce_unscaled: 0.0333 (0.0383)  loss_points_unscaled: 0.5538 (0.5753)\n",
            "[ep 356][lr 0.0000010][63.33s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0487 (0.0548)  loss_ce: 0.0377 (0.0432)  loss_points: 0.0114 (0.0116)  loss_ce_unscaled: 0.0377 (0.0432)  loss_points_unscaled: 0.5687 (0.5796)\n",
            "[ep 357][lr 0.0000010][64.04s]\n",
            "evaluation time 325.5335304737091\n",
            "=======================================test=======================================\n",
            "mae: 17.583333333333332 mse: 24.704771959598364 time: 325.5335304737091 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0416 (0.0553)  loss_ce: 0.0310 (0.0436)  loss_points: 0.0116 (0.0117)  loss_ce_unscaled: 0.0310 (0.0436)  loss_points_unscaled: 0.5784 (0.5848)\n",
            "[ep 358][lr 0.0000010][64.94s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0399 (0.0528)  loss_ce: 0.0284 (0.0411)  loss_points: 0.0115 (0.0117)  loss_ce_unscaled: 0.0284 (0.0411)  loss_points_unscaled: 0.5736 (0.5853)\n",
            "[ep 359][lr 0.0000010][63.90s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0412 (0.0545)  loss_ce: 0.0289 (0.0430)  loss_points: 0.0117 (0.0115)  loss_ce_unscaled: 0.0289 (0.0430)  loss_points_unscaled: 0.5864 (0.5766)\n",
            "[ep 360][lr 0.0000010][66.03s]\n",
            "evaluation time 302.5645990371704\n",
            "=======================================test=======================================\n",
            "mae: 17.704545454545453 mse: 22.846158751461257 time: 302.5645990371704 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000001  loss: 0.0438 (0.0552)  loss_ce: 0.0320 (0.0435)  loss_points: 0.0118 (0.0117)  loss_ce_unscaled: 0.0320 (0.0435)  loss_points_unscaled: 0.5887 (0.5835)\n",
            "[ep 361][lr 0.0000010][63.99s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0402 (0.0561)  loss_ce: 0.0290 (0.0444)  loss_points: 0.0113 (0.0117)  loss_ce_unscaled: 0.0290 (0.0444)  loss_points_unscaled: 0.5644 (0.5868)\n",
            "[ep 362][lr 0.0000010][63.81s]\n",
            "Averaged stats: lr: 0.000001  loss: 0.0510 (0.0580)  loss_ce: 0.0391 (0.0463)  loss_points: 0.0116 (0.0117)  loss_ce_unscaled: 0.0391 (0.0463)  loss_points_unscaled: 0.5824 (0.5840)\n",
            "[ep 363][lr 0.0000010][64.71s]\n",
            "evaluation time 324.49367213249207\n",
            "=======================================test=======================================\n",
            "mae: 17.931818181818183 mse: 24.640905876697033 time: 324.49367213249207 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000000  loss: 0.0445 (0.0511)  loss_ce: 0.0328 (0.0395)  loss_points: 0.0110 (0.0116)  loss_ce_unscaled: 0.0328 (0.0395)  loss_points_unscaled: 0.5518 (0.5801)\n",
            "[ep 364][lr 0.0000001][64.68s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0472 (0.0589)  loss_ce: 0.0370 (0.0475)  loss_points: 0.0114 (0.0115)  loss_ce_unscaled: 0.0370 (0.0475)  loss_points_unscaled: 0.5682 (0.5726)\n",
            "[ep 365][lr 0.0000001][66.02s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0489 (0.0568)  loss_ce: 0.0371 (0.0451)  loss_points: 0.0118 (0.0116)  loss_ce_unscaled: 0.0371 (0.0451)  loss_points_unscaled: 0.5883 (0.5819)\n",
            "[ep 366][lr 0.0000001][65.16s]\n",
            "evaluation time 336.37527775764465\n",
            "=======================================test=======================================\n",
            "mae: 20.204545454545453 mse: 28.103569276172127 time: 336.37527775764465 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000000  loss: 0.0433 (0.0572)  loss_ce: 0.0308 (0.0456)  loss_points: 0.0119 (0.0117)  loss_ce_unscaled: 0.0308 (0.0456)  loss_points_unscaled: 0.5943 (0.5826)\n",
            "[ep 367][lr 0.0000001][65.92s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0421 (0.0566)  loss_ce: 0.0318 (0.0449)  loss_points: 0.0116 (0.0117)  loss_ce_unscaled: 0.0318 (0.0449)  loss_points_unscaled: 0.5807 (0.5830)\n",
            "[ep 368][lr 0.0000001][65.08s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0394 (0.0519)  loss_ce: 0.0252 (0.0404)  loss_points: 0.0110 (0.0115)  loss_ce_unscaled: 0.0252 (0.0404)  loss_points_unscaled: 0.5508 (0.5768)\n",
            "[ep 369][lr 0.0000001][64.76s]\n",
            "evaluation time 282.07300066947937\n",
            "=======================================test=======================================\n",
            "mae: 18.303030303030305 mse: 23.092698570602945 time: 282.07300066947937 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000000  loss: 0.0451 (0.0540)  loss_ce: 0.0338 (0.0425)  loss_points: 0.0111 (0.0115)  loss_ce_unscaled: 0.0338 (0.0425)  loss_points_unscaled: 0.5563 (0.5732)\n",
            "[ep 370][lr 0.0000001][64.32s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0371 (0.0526)  loss_ce: 0.0244 (0.0410)  loss_points: 0.0114 (0.0116)  loss_ce_unscaled: 0.0244 (0.0410)  loss_points_unscaled: 0.5717 (0.5792)\n",
            "[ep 371][lr 0.0000001][63.80s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0474 (0.0554)  loss_ce: 0.0359 (0.0439)  loss_points: 0.0115 (0.0116)  loss_ce_unscaled: 0.0359 (0.0439)  loss_points_unscaled: 0.5768 (0.5776)\n",
            "[ep 372][lr 0.0000001][65.07s]\n",
            "evaluation time 317.4055185317993\n",
            "=======================================test=======================================\n",
            "mae: 17.257575757575758 mse: 23.109423500932568 time: 317.4055185317993 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000000  loss: 0.0413 (0.0495)  loss_ce: 0.0303 (0.0379)  loss_points: 0.0116 (0.0115)  loss_ce_unscaled: 0.0303 (0.0379)  loss_points_unscaled: 0.5785 (0.5770)\n",
            "[ep 373][lr 0.0000001][62.82s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0548 (0.0549)  loss_ce: 0.0436 (0.0433)  loss_points: 0.0112 (0.0116)  loss_ce_unscaled: 0.0436 (0.0433)  loss_points_unscaled: 0.5616 (0.5787)\n",
            "[ep 374][lr 0.0000001][65.22s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0483 (0.0490)  loss_ce: 0.0367 (0.0375)  loss_points: 0.0113 (0.0115)  loss_ce_unscaled: 0.0367 (0.0375)  loss_points_unscaled: 0.5631 (0.5772)\n",
            "[ep 375][lr 0.0000001][63.40s]\n",
            "evaluation time 314.7185959815979\n",
            "=======================================test=======================================\n",
            "mae: 17.53787878787879 mse: 23.249470505537943 time: 314.7185959815979 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000000  loss: 0.0405 (0.0501)  loss_ce: 0.0286 (0.0384)  loss_points: 0.0114 (0.0117)  loss_ce_unscaled: 0.0286 (0.0384)  loss_points_unscaled: 0.5701 (0.5844)\n",
            "[ep 376][lr 0.0000001][63.80s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0444 (0.0545)  loss_ce: 0.0337 (0.0428)  loss_points: 0.0115 (0.0117)  loss_ce_unscaled: 0.0337 (0.0428)  loss_points_unscaled: 0.5750 (0.5837)\n",
            "[ep 377][lr 0.0000001][64.68s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0418 (0.0518)  loss_ce: 0.0299 (0.0400)  loss_points: 0.0115 (0.0118)  loss_ce_unscaled: 0.0299 (0.0400)  loss_points_unscaled: 0.5737 (0.5909)\n",
            "[ep 378][lr 0.0000001][63.80s]\n",
            "evaluation time 318.9651896953583\n",
            "=======================================test=======================================\n",
            "mae: 18.189393939393938 mse: 25.065217962649985 time: 318.9651896953583 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000000  loss: 0.0405 (0.0536)  loss_ce: 0.0281 (0.0420)  loss_points: 0.0119 (0.0116)  loss_ce_unscaled: 0.0281 (0.0420)  loss_points_unscaled: 0.5963 (0.5808)\n",
            "[ep 379][lr 0.0000001][64.03s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0495 (0.0554)  loss_ce: 0.0378 (0.0434)  loss_points: 0.0122 (0.0119)  loss_ce_unscaled: 0.0378 (0.0434)  loss_points_unscaled: 0.6083 (0.5955)\n",
            "[ep 380][lr 0.0000001][64.45s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0413 (0.0558)  loss_ce: 0.0308 (0.0440)  loss_points: 0.0115 (0.0117)  loss_ce_unscaled: 0.0308 (0.0440)  loss_points_unscaled: 0.5766 (0.5875)\n",
            "[ep 381][lr 0.0000001][64.22s]\n",
            "evaluation time 313.9078223705292\n",
            "=======================================test=======================================\n",
            "mae: 17.454545454545453 mse: 23.1614884118993 time: 313.9078223705292 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000000  loss: 0.0407 (0.0525)  loss_ce: 0.0293 (0.0409)  loss_points: 0.0113 (0.0115)  loss_ce_unscaled: 0.0293 (0.0409)  loss_points_unscaled: 0.5660 (0.5771)\n",
            "[ep 382][lr 0.0000001][64.29s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0398 (0.0541)  loss_ce: 0.0290 (0.0426)  loss_points: 0.0110 (0.0115)  loss_ce_unscaled: 0.0290 (0.0426)  loss_points_unscaled: 0.5479 (0.5743)\n",
            "[ep 383][lr 0.0000001][64.84s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0422 (0.0548)  loss_ce: 0.0309 (0.0432)  loss_points: 0.0113 (0.0116)  loss_ce_unscaled: 0.0309 (0.0432)  loss_points_unscaled: 0.5645 (0.5793)\n",
            "[ep 384][lr 0.0000001][65.57s]\n",
            "evaluation time 305.0394914150238\n",
            "=======================================test=======================================\n",
            "mae: 17.25 mse: 21.96950365641179 time: 305.0394914150238 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000000  loss: 0.0479 (0.0593)  loss_ce: 0.0369 (0.0476)  loss_points: 0.0116 (0.0117)  loss_ce_unscaled: 0.0369 (0.0476)  loss_points_unscaled: 0.5785 (0.5828)\n",
            "[ep 385][lr 0.0000001][65.26s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0468 (0.0573)  loss_ce: 0.0351 (0.0457)  loss_points: 0.0112 (0.0116)  loss_ce_unscaled: 0.0351 (0.0457)  loss_points_unscaled: 0.5589 (0.5793)\n",
            "[ep 386][lr 0.0000001][64.50s]\n",
            "Averaged stats: lr: 0.000000  loss: 0.0460 (0.0548)  loss_ce: 0.0331 (0.0432)  loss_points: 0.0116 (0.0115)  loss_ce_unscaled: 0.0331 (0.0432)  loss_points_unscaled: 0.5813 (0.5773)\n",
            "[ep 387][lr 0.0000001][64.30s]\n",
            "evaluation time 325.89059567451477\n",
            "=======================================test=======================================\n",
            "mae: 18.87878787878788 mse: 26.1725046566048 time: 325.89059567451477 best mae: 14.522727272727273 at epoch: 64\n",
            "=======================================test=======================================\n",
            "Averaged stats: lr: 0.000000  loss: 0.0531 (0.0523)  loss_ce: 0.0400 (0.0407)  loss_points: 0.0116 (0.0116)  loss_ce_unscaled: 0.0400 (0.0407)  loss_points_unscaled: 0.5812 (0.5787)\n",
            "[ep 388][lr 0.0000001][63.53s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-78da1196e312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_max_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# record the training states after every epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-7433c9e2acd4>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, optimizer, device, epoch, max_norm)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# update logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "args = get_arguments()\n",
        "#\n",
        "#put the model path here if you have trained any or comment it out\n",
        "#args.resume = \"/content/drive/My Drive/P2PNet-Soy/ckpt_p2pnetSoy_testing/best_mae.pth\" \n",
        "# the directory to save the evaluations during training\n",
        "args.vis_dir = \"/content/drive/My Drive/P2PNet-Soy/vis_p2pnetSoy_testing\"\n",
        "if not os.path.exists(args.vis_dir):\n",
        "    os.makedirs(args.vis_dir)\n",
        "##\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{}'.format(args.gpu_id)\n",
        "# create the logging file\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)\n",
        "run_log_name = os.path.join(args.output_dir, 'run_log.txt')\n",
        "with open(run_log_name, \"w\") as log_file:\n",
        "    log_file.write('Eval Log %s\\n' % time.strftime(\"%c\"))\n",
        "#\n",
        "with open(run_log_name, \"a\") as log_file:\n",
        "    log_file.write(\"{}\".format(args))\n",
        "device = torch.device('cuda')\n",
        "# fix the seed for reproducibility\n",
        "seed = args.seed + get_rank()\n",
        "seed = args.seed\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "# get the P2PNet model\n",
        "model, criterion = build(args, training=True)\n",
        "# move to GPU\n",
        "model.to(device)\n",
        "criterion.to(device)\n",
        "\n",
        "model_without_ddp = model\n",
        "\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)\n",
        "# use different optimation params for different parts of the model\n",
        "param_dicts = [\n",
        "    {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"fpn\" not in n and p.requires_grad]},\n",
        "    {\n",
        "        \"params\": [p for n, p in model_without_ddp.named_parameters() if \"fpn\" in n and p.requires_grad],\n",
        "        \"lr\": args.lr_fpn,\n",
        "    },\n",
        "]\n",
        "# Adam is used by default\n",
        "optimizer = torch.optim.Adam(param_dicts, lr=args.lr)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
        "# create the training and valiation set\n",
        "train_set, val_set = loading_data(args.data_root)\n",
        "# create the sampler used during training\n",
        "sampler_train = torch.utils.data.RandomSampler(train_set)\n",
        "sampler_val = torch.utils.data.SequentialSampler(val_set)\n",
        "\n",
        "batch_sampler_train = torch.utils.data.BatchSampler(\n",
        "    sampler_train, args.batch_size, drop_last=True)\n",
        "# the dataloader for training\n",
        "data_loader_train = DataLoader(train_set, batch_sampler=batch_sampler_train,\n",
        "                                collate_fn=collate_fn_crowd, num_workers=args.num_workers)\n",
        "\n",
        "data_loader_val = DataLoader(val_set, 1, sampler=sampler_val,\n",
        "                                drop_last=False, collate_fn=collate_fn_crowd, num_workers=args.num_workers)\n",
        "\n",
        "if args.frozen_weights is not None:\n",
        "    checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
        "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
        "# resume the weights and training state if exists\n",
        "if args.resume:\n",
        "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
        "    model_without_ddp.load_state_dict(checkpoint['model'])\n",
        "    args.start_epoch = checkpoint['epoch']\n",
        "    new_start = 1\n",
        "    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "else:\n",
        "    new_start = 0\n",
        "##\n",
        "start_time = time.time()\n",
        "# save the performance during the training\n",
        "mae = []\n",
        "mse = []\n",
        "epoch_save = []\n",
        "# the logger writer\n",
        "writer = SummaryWriter(args.tensorboard_dir)\n",
        "# save latest weights every epoch\n",
        "if not os.path.exists(args.checkpoints_dir):\n",
        "    os.makedirs(args.checkpoints_dir)\n",
        "#\n",
        "step = 0\n",
        "# training starts here\n",
        "for epoch in range(args.start_epoch, args.epochs):\n",
        "    # always run evaluation first (to check what model has been loaded !!!)\n",
        "    if (epoch +2) % args.eval_freq == 0 or new_start: # and epoch != 0\n",
        "        # change the status right after the first iteration\n",
        "        new_start = 0\n",
        "        #\n",
        "        t1 = time.time()\n",
        "        result = evaluate_crowd_no_overlap(model, data_loader_val, device, epoch, args.threshold, args.vis_dir)\n",
        "        t2 = time.time()\n",
        "        print(\"evaluation time {}\".format(t2-t1))\n",
        "        mae.append(result[0])\n",
        "        mse.append(result[1])\n",
        "        epoch_save.append(epoch)\n",
        "        #\n",
        "        epoch_save_m = np.array(epoch_save)[mae == np.min(mae)][0]\n",
        "        # print the evaluation results\n",
        "        print('=======================================test=======================================')\n",
        "        print(\"mae:\", result[0], \"mse:\", result[1], \"time:\", t2 - t1, \"best mae:\", np.min(mae), \"at epoch: {}\".format(epoch_save_m) )\n",
        "        with open(run_log_name, \"a\") as log_file:\n",
        "            log_file.write(\"mae:{}, mse:{}, time:{}, best mae:{}\".format(result[0], \n",
        "                            result[1], t2 - t1, np.min(mae)))\n",
        "        print('=======================================test=======================================')\n",
        "        # recored the evaluation results\n",
        "        if writer is not None:\n",
        "            with open(run_log_name, \"a\") as log_file:\n",
        "                log_file.write(\"metric/mae@{}: {}\".format(step, result[0]))\n",
        "                log_file.write(\"metric/mse@{}: {}\".format(step, result[1]))\n",
        "            writer.add_scalar('metric/mae', result[0], step)\n",
        "            writer.add_scalar('metric/mse', result[1], step)\n",
        "            step += 1\n",
        "\n",
        "        # save the best model since begining\n",
        "        if abs(np.min(mae) - result[0]) < 0.01:\n",
        "            checkpoint_best_path = os.path.join(args.checkpoints_dir, 'best_mae.pth')\n",
        "            torch.save({\n",
        "                'model': model_without_ddp.state_dict(),\n",
        "                'epoch': epoch,\n",
        "            }, checkpoint_best_path)\n",
        "    ###\n",
        "    t1 = time.time()\n",
        "    stat = train_one_epoch(model, criterion, data_loader_train, optimizer, device, epoch, args.clip_max_norm)\n",
        "\n",
        "    # record the training states after every epoch\n",
        "    if writer is not None:\n",
        "        with open(run_log_name, \"a\") as log_file:\n",
        "            log_file.write(\"loss/loss@{}: {}\".format(epoch, stat['loss']))\n",
        "            log_file.write(\"loss/loss_ce@{}: {}\".format(epoch, stat['loss_ce']))\n",
        "            \n",
        "        writer.add_scalar('loss/loss', stat['loss'], epoch)\n",
        "        writer.add_scalar('loss/loss_ce', stat['loss_ce'], epoch)\n",
        "\n",
        "    t2 = time.time()\n",
        "    print('[ep %d][lr %.7f][%.2fs]' % \\\n",
        "            (epoch, optimizer.param_groups[0]['lr'], t2 - t1))\n",
        "    with open(run_log_name, \"a\") as log_file:\n",
        "        log_file.write('[ep %d][lr %.7f][%.2fs]' % (epoch, optimizer.param_groups[0]['lr'], t2 - t1))\n",
        "    # change lr according to the scheduler\n",
        "    lr_scheduler.step()\n",
        "    #\n",
        "    # save latest weights every epoch\n",
        "    checkpoint_latest_path = os.path.join(args.checkpoints_dir, 'latest.pth')\n",
        "    torch.save({\n",
        "        'model': model_without_ddp.state_dict(),\n",
        "        'epoch': epoch,\n",
        "    }, checkpoint_latest_path)\n",
        "    ## clear the cell output regulary\n",
        "    if epoch % 150 == 0 and epoch != 0:\n",
        "        clear_output()\n",
        "# total time for training\n",
        "total_time = time.time() - start_time\n",
        "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "print('Training time {}'.format(total_time_str))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a4L6DvrcUdzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AydizQosJs-B"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmJDLCM9ArSQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdvepId9e2wJ"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "# create the P2PNet model\n",
        "def build_eval(args):\n",
        "    # treats persons as a single class\n",
        "    num_classes = 1\n",
        "    model = P2PNet(args.row, args.line)\n",
        "    return model"
      ],
      "metadata": {
        "id": "PHwrhTVD5b32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCIFstqge11V"
      },
      "outputs": [],
      "source": [
        "def get_arguments():\n",
        "    \"\"\"Parse all the arguments provided from the CLI.\n",
        "    Returns:\n",
        "      A list of parsed arguments.\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"Object Counting Framework\")\n",
        "\n",
        "    parser.add_argument('--threshold', default=0.5, type=float,\n",
        "                        help=\"threshold in evalluation: evaluate_crowd_no_overlap\")\n",
        "    parser.add_argument('--row', default=2, type=int,\n",
        "                        help=\"row number of anchor points\")\n",
        "    parser.add_argument('--line', default=2, type=int,\n",
        "                        help=\"line number of anchor points\")\n",
        "    parser.add_argument('--test_root', default='/content/drive/My Drive/P2PNet-Soy/Test_data',\n",
        "                        help='path where the dataset is')\n",
        "    parser.add_argument('--seed', default=42, type=int)\n",
        "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
        "    parser.add_argument('--num_workers', default=1, type=int)\n",
        "    parser.add_argument('--gpu_id', default=0, type=int, help='the gpu used for training')\n",
        "    #\n",
        "    opt = parser.parse_known_args()[0] #if known else parser.parse_args()\n",
        "    return opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9BNYRIPe14r"
      },
      "outputs": [],
      "source": [
        "# specify the directories of the saved model\n",
        "args = get_arguments()\n",
        "args.resume = \"/content/drive/My Drive/P2PNet-Soy/ckpt_p2pnetSoy_testing/best_mae.pth\" \n",
        "args.pred_dir = \"/content/drive/My Drive/P2PNet-Soy/prediction_on_Test_data\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQA5wI7Nt4KD"
      },
      "outputs": [],
      "source": [
        "####\n",
        "if not os.path.exists(args.pred_dir):\n",
        "    os.makedirs(args.pred_dir)\n",
        "##\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{}'.format(args.gpu_id)\n",
        "\n",
        "device = torch.device('cuda')\n",
        "# fix the seed for reproducibility\n",
        "seed = args.seed + get_rank()\n",
        "seed = args.seed\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "# get the P2PNet model\n",
        "model = build_eval(args)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# load trained model\n",
        "if args.resume is not None:\n",
        "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "#\n",
        "model.eval()\n",
        "#\n",
        "step = 0\n",
        "epoch = 0\n",
        "###\n",
        "# create the pre-processing transform\n",
        "transform = standard_transforms.Compose([\n",
        "    standard_transforms.ToTensor(), \n",
        "    standard_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8UudUNVfl8q"
      },
      "outputs": [],
      "source": [
        "# start prediction on new images\n",
        "\n",
        "# set your image path here\n",
        "folder_path = args.test_root \n",
        "img_list = glob.glob(os.path.join(folder_path,'*.JPG'))#[:3]\n",
        "\n",
        "#\n",
        "# save the number of detected pods\n",
        "csv_name = (folder_path).split(\"/\")[-1]\n",
        "loss_csv = open(os.path.join(args.pred_dir, '{}_seed_counting.csv'.format(csv_name)), 'w+')\n",
        "#\n",
        "for img_path in img_list:\n",
        "\n",
        "    print(\"image path = {}\".format(img_path))\n",
        "    # load the images\n",
        "    img_00 = cv2.imread(img_path)\n",
        "    # reduce the size by 2\n",
        "    img_0 = cv2.resize(img_00, (int(img_00.shape[1]/2), int(img_00.shape[0]/2)), interpolation = cv2.INTER_AREA)\n",
        "    # round the size\n",
        "    height, width = img_0.shape[:2]\n",
        "\n",
        "    new_width = (width // 256 +1) * 256\n",
        "    new_height = (height // 256 + 1)* 256\n",
        "    new_img = np.ones((new_height, new_width,3)).astype(np.uint8)*255\n",
        "    new_img[:height,:width,:] = img_0\n",
        "    #\n",
        "    img_name = img_path.split(\"/\")[-1]\n",
        "    # prepare the empty images to save the predictions without and with k-d tree cluster point mergence\n",
        "    img_name_pred_before = img_name.replace(\".JPG\", '_pred_bf.JPG')\n",
        "    img_name_pred_after = img_name.replace(\".JPG\", '_pred_af.JPG')\n",
        "    # \n",
        "    img_raw = cv2.cvtColor(new_img, cv2.COLOR_BGR2RGB)\n",
        "    ## \n",
        "    img_raw_or_bf= new_img.copy()\n",
        "    img_raw_or_af= new_img.copy()\n",
        "    #\n",
        "    # divide the image into two to save memory\n",
        "    img_to_draw_before = np.array(img_raw_or_bf)\n",
        "    img_to_draw_after = np.array(img_raw_or_af)\n",
        "    # divide the image to save the ram\n",
        "    new_width_hf = int(new_width/4)\n",
        "    new_height_hf = int(new_height/4)\n",
        "    for shi in range(1,5):\n",
        "        print(\"the {} slice\".format(shi))\n",
        "        #\n",
        "        img_raw_shi = Image.fromarray(cv2.cvtColor(new_img[:, new_width_hf*(shi-1):new_width_hf*shi,:], cv2.COLOR_BGR2RGB))\n",
        "        # pre-proccessing\n",
        "        img_shi = transform(img_raw_shi)\n",
        "\n",
        "        samples = torch.Tensor(img_shi).unsqueeze(0)\n",
        "        samples = samples.to(device)\n",
        "        # run inference\n",
        "        outputs = model(samples)\n",
        "\n",
        "        outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]\n",
        "\n",
        "        outputs_points = outputs['pred_points'][0]\n",
        "\n",
        "        # filter the predictions\n",
        "        points = outputs_points[outputs_scores > args.threshold].detach().cpu().numpy()#.tolist()\n",
        "        if points.shape[0] > 0:\n",
        "            cutoff = 10\n",
        "            components = nx.connected_components(\n",
        "                nx.from_edgelist(\n",
        "                    (i, j) for i, js in enumerate(\n",
        "                        spatial.KDTree(points).query_ball_point(points, cutoff)\n",
        "                    )\n",
        "                    for j in js\n",
        "                )\n",
        "            )\n",
        "\n",
        "            clusters = {j: i for i, js in enumerate(components) for j in js}\n",
        "\n",
        "            # reorganize the points to the order of clusters \n",
        "            points_reo = np.zeros(points.shape)\n",
        "            i = 0\n",
        "            for key in clusters.keys():\n",
        "                points_reo[i,:] = points[key,:]\n",
        "                i+=1\n",
        "            # points_n has the same order as clusters\n",
        "            res = [clusters[key] for key in clusters.keys()]\n",
        "            res_n = np.array(res).reshape(-1,1)\n",
        "\n",
        "            points_n = []\n",
        "            for i in np.unique(res_n):\n",
        "                tmp = points_reo[np.where(res_n[:,0] == i)]\n",
        "                points_n.append( [np.mean(tmp[:,0]), np.mean(tmp[:,1])])\n",
        "        else:\n",
        "            points_n = points.tolist()\n",
        "        #\n",
        "        if points_n:\n",
        "            if shi ==1:\n",
        "                points_bf_sum = np.array(points)\n",
        "                points_af_sum = np.array(points_n)\n",
        "                print(\"points_af_sum {}\".format(points_af_sum.shape))\n",
        "            else:\n",
        "                points_bf_sum = np.concatenate((points_bf_sum, np.array(points)), 0)\n",
        "                points_af_sum = np.concatenate((points_af_sum, np.array(points_n)), 0)\n",
        "        \n",
        "        # draw the predictions\n",
        "        #. before point mergence\n",
        "        size = 5\n",
        "        img_to_draw_before_in = img_to_draw_before[:, new_width_hf*(shi-1):new_width_hf*shi,:]\n",
        "        img_to_draw_before_in_n = img_to_draw_before_in.copy()\n",
        "        for p in points:\n",
        "            img_to_draw_before_in_n = cv2.circle(img_to_draw_before_in_n, (int(p[0]), int(p[1])), size, (255, 0, 0), -1)\n",
        "        #\n",
        "        alpha = 0.6  # Transparency factor.\n",
        "        # \n",
        "        img_to_draw_before_in_nn = cv2.addWeighted(img_to_draw_before_in_n, alpha, img_to_draw_before_in, 1 - alpha, 0)\n",
        "        # save the visualized image\n",
        "        img_to_draw_before[:,new_width_hf*(shi-1):new_width_hf*shi,:] = img_to_draw_before_in_nn\n",
        "        #. after  \n",
        "        size = 5\n",
        "        img_to_draw_after_in = img_to_draw_after[:, new_width_hf*(shi-1):new_width_hf*shi,:]\n",
        "        img_to_draw_after_in_n= img_to_draw_after_in.copy()\n",
        "        for p in points_n:\n",
        "            img_to_draw_after_in_n = cv2.circle(img_to_draw_after_in_n, (int(p[0]), int(p[1])), size, (255, 0, 0), -1)\n",
        "        #\n",
        "        # \n",
        "        img_to_draw_after_in_nn = cv2.addWeighted(img_to_draw_after_in_n, alpha, img_to_draw_after_in, 1 - alpha, 0)\n",
        "            #\n",
        "        img_to_draw_after[:,new_width_hf*(shi-1):new_width_hf*shi,:] = img_to_draw_after_in_nn\n",
        "    #\n",
        "    #predict_cnt = int((outputs_scores > threshold).sum())\n",
        "    points_bf_sum = points_bf_sum.tolist()\n",
        "    points_af_sum = points_af_sum.tolist()\n",
        "    predict_cnt_before = len(points_bf_sum)\n",
        "    predict_cnt_after = len(points_af_sum)\n",
        "    #\n",
        "    # Using cv2.putText() method\n",
        "    x, y, w, h = 20, 20, 1250, 140\n",
        "    sub_img = img_to_draw_before[y:y+h, x:x+w]\n",
        "    white_rect = np.ones(sub_img.shape, dtype=np.uint8) * 255\n",
        "    res = cv2.addWeighted(sub_img, 0.5, white_rect, 0.5, 1.0)\n",
        "    img_to_draw_before_in = img_to_draw_before.copy()\n",
        "    img_to_draw_before_in[y:y+h, x:x+w] = res\n",
        "    ##\n",
        "    img_to_draw_before_out = cv2.putText(img_to_draw_before_in,     \"P2PNet-Soy: {}\".format(predict_cnt_before), (50, 130), cv2.FONT_HERSHEY_SIMPLEX, 4, (255, 0, 0), 8, cv2.LINE_AA)\n",
        "    ##\n",
        "    x, y, w, h = 20, 20, 1150, 140\n",
        "    sub_img = img_to_draw_after[y:y+h, x:x+w]\n",
        "    white_rect = np.ones(sub_img.shape, dtype=np.uint8) * 255\n",
        "    res = cv2.addWeighted(sub_img, 0.5, white_rect, 0.5, 1.0)\n",
        "    img_to_draw_after_in = img_to_draw_after.copy()\n",
        "    img_to_draw_after_in[y:y+h, x:x+w] = res\n",
        "    #\n",
        "    img_to_draw_after_out = cv2.putText(img_to_draw_after_in,       \"P2PNet-Soy: {}\".format(predict_cnt_after), (30, 130), cv2.FONT_HERSHEY_SIMPLEX, 4, (255, 0, 0), 8, cv2.LINE_AA)\n",
        "    # save everything in the same folder\n",
        "    path_out = args.pred_dir\n",
        "    # save the visualized image\n",
        "    cv2.imwrite(os.path.join(path_out, img_name_pred_before), img_to_draw_before_out[:height,:width,:])\n",
        "    cv2.imwrite(os.path.join(path_out, img_name_pred_after), img_to_draw_after_out[:height,:width,:])\n",
        "    #\n",
        "    print(\"Number of seeds before = {}\".format(predict_cnt_before))\n",
        "    print(\"Number of seeds after = {}\".format(predict_cnt_after))\n",
        "    # save the detected pod number\n",
        "    loss_csv.write('{},{},{},{}\\n'.format(img_name_pred_before.split(\".\")[0], predict_cnt_before, img_name_pred_after.split(\".\")[0], predict_cnt_after))\n",
        "    loss_csv.flush()  \n",
        "loss_csv.close"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "P2PNet-Soy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}