{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KwBYMnSdD4hX","outputId":"55c48dd1-cc32-4953-ce11-14fb1429a64b","executionInfo":{"status":"ok","timestamp":1667900177339,"user_tz":-540,"elapsed":13883,"user":{"displayName":"jiangsan zhao","userId":"15833650277394623841"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: hdf5storage in /usr/local/lib/python3.7/dist-packages (0.1.18)\n","Requirement already satisfied: h5py>=2.1 in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (3.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (1.21.6)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.1->hdf5storage) (1.5.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Requirement already satisfied: torch==1.8.0+cu111 in /usr/local/lib/python3.7/dist-packages (1.8.0+cu111)\n","Requirement already satisfied: torchvision==0.9.0+cu111 in /usr/local/lib/python3.7/dist-packages (0.9.0+cu111)\n","Requirement already satisfied: torchaudio==0.8.0 in /usr/local/lib/python3.7/dist-packages (0.8.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (4.1.1)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.0+cu111) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.0+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n","Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.8.0+cu111)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->torchvision) (4.1.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.5.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n","Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/nightly/cu101/torch-1.7.0.dev20200626%2Bcu101-cp36-cp36m-linux_x86_64.whl\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"]}],"source":["#!pip install --no-cache-dir -I pillow\n","!pip install hdf5storage\n","#!pip install http://download.pytorch.org/whl/cu92/torch-1.6.0-cp36-cp36m-linux_x86_64.whl\n","#!pip install torch==1.8.0 torchvision==0.9.0 -qq # this line need to run the first time and then restart runtime to make the whole script work well\n","!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","!pip3 install torchvision\n","#!git clone https://github.com/lanpa/tensorboardX && cd tensorboardX && python setup.py install\n","!pip install tensorboardX\n","!pip install --pre torch -f  https://download.pytorch.org/whl/nightly/cu101/torch-1.7.0.dev20200626%2Bcu101-cp36-cp36m-linux_x86_64.whl\n","#!git clone https://github.com/acecreamu/deep-hs-prior"]},{"cell_type":"code","source":[],"metadata":{"id":"VM_cWYaegOBt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1laN8KjnD5Lv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c6c2287-0af3-4862-9d49-402ee45c0a29","executionInfo":{"status":"ok","timestamp":1667900181542,"user_tz":-540,"elapsed":4208,"user":{"displayName":"jiangsan zhao","userId":"15833650277394623841"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","#os.chdir(\"/content/drive/My Drive/maize_season3_RededgeMultispectral_20181030_10m_flight2/\") # folder containing your images used for training and testing \n","#os.chdir(\"/content/drive/My Drive/maize_season3_RededgeMultispectral_20181119_10m/\") # folder containing your images used for training and testing \n","os.chdir(\"/content/drive/My Drive\")\n","path=os.getcwd() "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmokxdZSqBiW","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"19ddf875-1cda-4838-998f-55615e2f7060","executionInfo":{"status":"ok","timestamp":1667900181543,"user_tz":-540,"elapsed":19,"user":{"displayName":"jiangsan zhao","userId":"15833650277394623841"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/CrowdCounting-P2PNet'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}],"source":["# !git clone https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet\n","os.chdir(\"/content/drive/My Drive/CrowdCounting-P2PNet\")\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-AvMARNKP1G"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2dc1-u6KMTS"},"outputs":[],"source":["## sha.py\n","import os\n","import random\n","\n","from scipy import spatial\n","import networkx as nx\n","\n","import torch\n","import numpy as np\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import cv2\n","import glob\n","import scipy.io as io\n","from matplotlib import pyplot as plt\n","plt.switch_backend('agg')\n","\n","class SHHA(Dataset):\n","    def __init__(self, data_root, transform=None, train=False, patch=False, flip=False):\n","        self.root_path = data_root\n","        self.train_lists = os.path.join(self.root_path, \"soypod_crop_counting_a.txt\")\n","        self.eval_list = os.path.join(self.root_path, \"soypod_crop_counting_b.txt\")\n","        # there may exist multiple list files\n","        #self.img_list_file = [name.split(',') for name in open(self.train_lists).read().splitlines()]\n","         \n","        if train:\n","            self.img_list_file = [name.split(',') for name in open(self.train_lists).read().splitlines()]\n","        else:\n","            self.img_list_file = [name.split(',') for name in open(self.eval_list).read().splitlines()]\n","\n","        self.img_list = self.img_list_file\n","        \n","        # \n","        self.nSamples = len(self.img_list)\n","        \n","        self.transform = transform\n","        self.train = train\n","        self.patch = patch\n","        self.flip = flip\n","\n","    def __len__(self):\n","        return self.nSamples\n","\n","    def __getitem__(self, index):\n","        assert index <= len(self), 'index range error'\n","\n","        img_path = self.img_list[index][0]\n","        gt_path = self.img_list[index][1]\n","        # \n","        img, point = load_data((img_path, gt_path), self.train)\n","        #\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        if self.train:\n","            # data augmentation -> random scale\n","            scale_range = [0.5, 1.4]\n","            min_size = min(img.shape[1:])\n","            scale = random.uniform(*scale_range)\n","            # scale the image and points\n","            if scale * min_size > 224:\n","                img = torch.nn.functional.upsample_bilinear(img.unsqueeze(0), scale_factor=scale).squeeze(0)\n","                point *= scale\n","        # random crop augumentaiton\n","        if self.train and self.patch:\n","            img, point = random_crop(img, point)\n","            for i, _ in enumerate(point):\n","                point[i] = torch.Tensor(point[i])\n","        # random flipping\n","        if random.random() > 0.1 and self.train and self.flip: # never flip\n","            # random flip\n","            img = torch.Tensor(img[:, :, :, ::-1].copy())\n","            for i, _ in enumerate(point):\n","                point[i][:, 0] = 224 - point[i][:, 0]\n","        # random change brightness\n","        if random.random() > 0.3 and self.train: # never flip\n","            #\n","            img = (torch.Tensor(img).clone())*random.uniform(8,12)/10\n","            for i, _ in enumerate(point):\n","                point[i][:, 0] = point[i][:, 0]\n","\n","        if not self.train:\n","            point = [point]\n","\n","        img = torch.Tensor(img)\n","        #  need to adapt your own image names\n","        target = [{} for i in range(len(point))]\n","        for i, _ in enumerate(point):\n","            target[i]['point'] = torch.Tensor(point[i])\n","            image_id_1 = int(img_path.split('/')[-1].split('.')[0].split(\"_\")[1][4:8])\n","            image_id_1 = torch.Tensor([image_id_1]).long()\n","            #\n","            image_id_2 = int(img_path.split('/')[-1].split('.')[0].split(\"_\")[3])\n","            image_id_2 = torch.Tensor([image_id_2]).long()\n","            target[i]['image_id_1'] = image_id_1\n","            target[i]['image_id_2'] = image_id_2\n","            target[i]['labels'] = torch.ones([point[i].shape[0]]).long()\n","\n","        return img, target\n","\n","\n","def load_data(img_gt_path, train):\n","    img_path, gt_path = img_gt_path\n","    # load the images\n","    img = cv2.imread(img_path)\n","    img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","    # load ground truth points\n","    points = []\n","    #\n","    pts = open(gt_path).read().splitlines()\n","    for pt_0 in pts:\n","        pt = eval(pt_0)        \n","        x = float(pt[0])\n","        y = float(pt[1])\n","        points.append([x, y])\n","    return img, np.array(points)\n","\n","# random crop augumentation\n","def random_crop(img, den, num_patch=10):\n","    half_h = 224\n","    half_w = 224\n","    result_img = np.zeros([num_patch, img.shape[0], half_h, half_w])\n","    result_den = []\n","    # \n","    for i in range(num_patch):\n","        start_h = random.randint(0, img.size(1) - half_h)\n","        start_w = random.randint(0, img.size(2) - half_w)\n","        end_h = start_h + half_h\n","        end_w = start_w + half_w\n","        # \n","        result_img[i] = img[:, start_h:end_h, start_w:end_w]#*random.uniform(5,15)/10\n","        # copy the cropped points\n","        idx = (den[:, 0] >= start_w) & (den[:, 0] <= end_w) & (den[:, 1] >= start_h) & (den[:, 1] <= end_h)\n","        # \n","        record_den = den[idx]\n","        record_den[:, 0] -= start_w\n","        record_den[:, 1] -= start_h\n","\n","        result_den.append(record_den)\n","\n","    return result_img, result_den"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXVt7zseriim"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZHJfljJc-bb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fkDa6XEKMWE"},"outputs":[],"source":["# load_data.py\n","import torchvision.transforms as standard_transforms\n","\n","# DeNormalize used to get original images\n","class DeNormalize(object):\n","    def __init__(self, mean, std):\n","        self.mean = mean\n","        self.std = std\n","\n","    def __call__(self, tensor):\n","        for t, m, s in zip(tensor, self.mean, self.std):\n","            t.mul_(s).add_(m)\n","        return tensor\n","\n","def loading_data(data_root):\n","    # the pre-proccssing transform\n","    transform = standard_transforms.Compose([\n","        standard_transforms.ToTensor(), \n","        standard_transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                    std=[0.229, 0.224, 0.225]),\n","    ])\n","    # create the training dataset\n","    train_set = SHHA(data_root, train=True, transform=transform, patch=True, flip=True)\n","    # create the validation dataset\n","    val_set = SHHA(data_root, train=False, transform=transform)\n","\n","    return train_set, val_set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63ey3OkobVMQ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wZY5NX0-KMZO"},"outputs":[],"source":["# engine.py\n","import math\n","import os\n","import sys\n","from typing import Iterable\n","\n","import torch\n","\n","#import util.misc as utils\n","import numpy as np\n","import time\n","import torchvision.transforms as standard_transforms\n","import cv2\n","\n","class DeNormalize(object):\n","    def __init__(self, mean, std):\n","        self.mean = mean\n","        self.std = std\n","\n","    def __call__(self, tensor):\n","        for t, m, s in zip(tensor, self.mean, self.std):\n","            t.mul_(s).add_(m)\n","        return tensor\n","#\n","def gen_random_scale_n(img, rnd=3):\n","    np.random.seed(rnd)\n","    scale = torch.tensor(np.random.uniform(0.1, 1.91, (1,1, img.size()[2],img.size()[3]))).float()  \n","    return torch.mul(img, scale)\n","##\n","def vis(samples, targets, pred, vis_dir, epoch, predict_cnt, gt_cnt):\n","    '''\n","    samples -> tensor: [batch, 3, H, W]\n","    targets -> list of dict: [{'points':[], 'image_id': str}]\n","    pred -> list: [num_preds, 2]\n","    '''\n","    gts = [t['point'].tolist() for t in targets]\n","\n","    pil_to_tensor = standard_transforms.ToTensor()\n","\n","    restore_transform = standard_transforms.Compose([\n","        DeNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        standard_transforms.ToPILImage()\n","    ])\n","    # draw one by one\n","    for idx in range(samples.shape[0]):\n","        sample = restore_transform(samples[idx])\n","        sample = pil_to_tensor(sample.convert('RGB')).numpy() * 255\n","        sample_gt = sample.transpose([1, 2, 0])[:, :, :].astype(np.uint8).copy()\n","        sample_pred = sample.transpose([1, 2, 0])[:, :, :].astype(np.uint8).copy()\n","\n","        max_len = np.max(sample_gt.shape)\n","\n","        size = 5\n","        # draw gt\n","        for t in gts[idx]:\n","            sample_gt = cv2.circle(sample_gt, (int(t[0]), int(t[1])), size, (0, 255, 0), -1)\n","            #print(\"sample_gt {}\".format(sample_gt.shape))\n","        # draw predictions\n","        for p in pred[idx]:\n","            sample_pred = cv2.circle(sample_pred, (int(p[0]), int(p[1])), size, (0, 0, 255), -1)\n","\n","        name_1 = targets[idx]['image_id_1']\n","        name_2 = targets[idx]['image_id_2']\n","        #################\n","        fig = plt.figure()\n","        ax1 = fig.add_subplot(1, 2, 1)\n","        ax1.imshow(sample_gt)\n","        ax1.get_xaxis().set_visible(False)\n","        ax1.get_yaxis().set_visible(False)\n","        ax2 = fig.add_subplot(1, 2, 2)\n","        ax2.imshow(sample_pred)\n","        ax2.get_xaxis().set_visible(False)\n","        ax2.get_yaxis().set_visible(False)\n","        fig.suptitle('manual count=%4.2f, inferred count=%4.2f'%(gt_cnt, predict_cnt), fontsize=10)\n","        plt.tight_layout(rect=[0, 0, 0.95, 0.95]) # maize tassels counting\n","        plt.savefig(os.path.join(vis_dir, '{}_{}_id_{}_ind_{}.jpg'.format(epoch, idx, int(name_1), int(name_2))), bbox_inches='tight', dpi = 300)\n","        plt.close()\n","\n","\n","# the training routine\n","def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n","                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n","                    device: torch.device, epoch: int, max_norm: float = 0):\n","    model.train()\n","    criterion.train()\n","    metric_logger = MetricLogger(delimiter=\"  \")\n","    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n","    # iterate all training samples\n","    for samples, targets in data_loader:\n","        #print(\"samples {}\".format(samples.size()))\n","        #samples = gen_random_scale_n(samples)\n","        #\n","        samples = samples.to(device)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","        # forward\n","        outputs = model(samples)\n","        #\n","        #print(\"outputs\")\n","        #print(outputs)\n","        # calc the losses\n","        loss_dict = criterion(outputs, targets)\n","        weight_dict = criterion.weight_dict\n","        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n","\n","        # reduce all losses (get the mean values)\n","        loss_dict_reduced = reduce_dict(loss_dict)\n","        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n","                                      for k, v in loss_dict_reduced.items()}\n","        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n","                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n","        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n","\n","        loss_value = losses_reduced_scaled.item()\n","\n","        if not math.isfinite(loss_value):\n","            print(\"Loss is {}, stopping training\".format(loss_value))\n","            print(loss_dict_reduced)\n","            sys.exit(1)\n","        # backward\n","        optimizer.zero_grad()\n","        losses.backward()\n","        if max_norm > 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n","        optimizer.step()\n","        # update logger\n","        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n","        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n","    # gather the stats from all processes\n","    metric_logger.synchronize_between_processes()\n","    print(\"Averaged stats:\", metric_logger)\n","    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n","\n","# the inference routine\n","@torch.no_grad()\n","def evaluate_crowd_no_overlap(model, data_loader, device, epoch, threshold, vis_dir=None):\n","    model.eval()\n","\n","    metric_logger = MetricLogger(delimiter=\"  \")\n","    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n","    # run inference on all images to calc MAE\n","    maes = []\n","    mses = []\n","    for samples, targets in data_loader:\n","\n","        samples = samples.to(device)\n","\n","        outputs = model(samples)\n","        outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]\n","\n","        outputs_points = outputs['pred_points'][0]\n","\n","        gt_cnt = targets[0]['point'].shape[0]\n","        # 0.5 is used by default\n","        threshold = threshold\n","\n","        points = outputs_points[outputs_scores > threshold].detach().cpu().numpy()#.tolist()\n","        #print(\"points {}\".format(points))\n","        #print(\"points {}\".format(points.shape[0]))\n","        if points.shape[0]< 1000 and points.shape[0] != 0:\n","            #print(\"doing clustering\")\n","            cutoff = 500/points.shape[0]\n","            if cutoff<20:\n","                cutoff = 20\n","            components = nx.connected_components(\n","                nx.from_edgelist(\n","                    (i, j) for i, js in enumerate(\n","                        spatial.KDTree(points).query_ball_point(points, cutoff)\n","                    )\n","                    for j in js\n","                )\n","            )\n","\n","            clusters = {j: i for i, js in enumerate(components) for j in js}\n","\n","            # reorganize the points to the order of clusters \n","            points_reo = np.zeros(points.shape)\n","            i = 0\n","            for key in clusters.keys():\n","                #print(key)\n","                points_reo[i,:] = points[key,:]\n","                i+=1\n","            # points_n has the same order as clusters\n","            res = [clusters[key] for key in clusters.keys()]\n","            res_n = np.array(res).reshape(-1,1)\n","\n","            points_n = []\n","            for i in np.unique(res_n):\n","                tmp = points_reo[np.where(res_n[:,0] == i)]\n","                points_n.append( [np.mean(tmp[:,0]), np.mean(tmp[:,1])])\n","        else:\n","            points_n = points.tolist()\n","        #print(\"points_n {}\".format(len(points_n)))\n","        # calculate the distance and find the center of the too close points\n","\n","        #predict_cnt = int((outputs_scores > threshold).sum())\n","        predict_cnt = len(points_n)\n","        # if specified, save the visualized images\n","        if vis_dir is not None: \n","            vis(samples, targets, [points_n], vis_dir, epoch, predict_cnt, gt_cnt)\n","        # accumulate MAE, MSE\n","        mae = abs(predict_cnt - gt_cnt)\n","        mse = (predict_cnt - gt_cnt) * (predict_cnt - gt_cnt)\n","        maes.append(float(mae))\n","        mses.append(float(mse))\n","    # calc MAE, MSE\n","    mae = np.mean(maes)\n","    mse = np.sqrt(np.mean(mses))\n","\n","    return mae, mse"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zVTZ57LPKMdb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yB4BETJp2m6W"},"outputs":[],"source":["import argparse\n","import datetime\n","import random\n","import time\n","from pathlib import Path\n","from IPython.display import clear_output \n","\n","import torch\n","from torch.utils.data import DataLoader, DistributedSampler\n","\n","#from models import build_model\n","import os\n","from tensorboardX import SummaryWriter\n","import warnings\n","warnings.filterwarnings('ignore')\n"]},{"cell_type":"code","source":["## misc\n","import os\n","import subprocess\n","import time\n","from collections import defaultdict, deque\n","import datetime\n","import pickle\n","from typing import Optional, List\n","\n","import torch\n","import torch.distributed as dist\n","from torch import Tensor\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","# needed due to empty tensor bug in pytorch and torchvision 0.5\n","import torchvision\n","if float(torchvision.__version__[:3]) < 0.7:\n","    from torchvision.ops import _new_empty_tensor\n","    from torchvision.ops.misc import _output_size\n","\n","\n","class SmoothedValue(object):\n","    \"\"\"Track a series of values and provide access to smoothed values over a\n","    window or the global series average.\n","    \"\"\"\n","\n","    def __init__(self, window_size=20, fmt=None):\n","        if fmt is None:\n","            fmt = \"{median:.4f} ({global_avg:.4f})\"\n","        self.deque = deque(maxlen=window_size)\n","        self.total = 0.0\n","        self.count = 0\n","        self.fmt = fmt\n","\n","    def update(self, value, n=1):\n","        self.deque.append(value)\n","        self.count += n\n","        self.total += value * n\n","\n","    def synchronize_between_processes(self):\n","        \"\"\"\n","        Warning: does not synchronize the deque!\n","        \"\"\"\n","        if not is_dist_avail_and_initialized():\n","            return\n","        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n","        dist.barrier()\n","        dist.all_reduce(t)\n","        t = t.tolist()\n","        self.count = int(t[0])\n","        self.total = t[1]\n","\n","    @property\n","    def median(self):\n","        d = torch.tensor(list(self.deque))\n","        return d.median().item()\n","\n","    @property\n","    def avg(self):\n","        d = torch.tensor(list(self.deque), dtype=torch.float32)\n","        return d.mean().item()\n","\n","    @property\n","    def global_avg(self):\n","        return self.total / self.count\n","\n","    @property\n","    def max(self):\n","        return max(self.deque)\n","\n","    @property\n","    def value(self):\n","        return self.deque[-1]\n","\n","    def __str__(self):\n","        return self.fmt.format(\n","            median=self.median,\n","            avg=self.avg,\n","            global_avg=self.global_avg,\n","            max=self.max,\n","            value=self.value)\n","\n","\n","def all_gather(data):\n","    \"\"\"\n","    Run all_gather on arbitrary picklable data (not necessarily tensors)\n","    Args:\n","        data: any picklable object\n","    Returns:\n","        list[data]: list of data gathered from each rank\n","    \"\"\"\n","    world_size = get_world_size()\n","    if world_size == 1:\n","        return [data]\n","\n","    # serialized to a Tensor\n","    buffer = pickle.dumps(data)\n","    storage = torch.ByteStorage.from_buffer(buffer)\n","    tensor = torch.ByteTensor(storage).to(\"cuda\")\n","\n","    # obtain Tensor size of each rank\n","    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n","    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n","    dist.all_gather(size_list, local_size)\n","    size_list = [int(size.item()) for size in size_list]\n","    max_size = max(size_list)\n","\n","    # receiving Tensor from all ranks\n","    # we pad the tensor because torch all_gather does not support\n","    # gathering tensors of different shapes\n","    tensor_list = []\n","    for _ in size_list:\n","        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n","    if local_size != max_size:\n","        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n","        tensor = torch.cat((tensor, padding), dim=0)\n","    dist.all_gather(tensor_list, tensor)\n","\n","    data_list = []\n","    for size, tensor in zip(size_list, tensor_list):\n","        buffer = tensor.cpu().numpy().tobytes()[:size]\n","        data_list.append(pickle.loads(buffer))\n","\n","    return data_list\n","\n","\n","def reduce_dict(input_dict, average=True):\n","    \"\"\"\n","    Args:\n","        input_dict (dict): all the values will be reduced\n","        average (bool): whether to do average or sum\n","    Reduce the values in the dictionary from all processes so that all processes\n","    have the averaged results. Returns a dict with the same fields as\n","    input_dict, after reduction.\n","    \"\"\"\n","    world_size = get_world_size()\n","    if world_size < 2:\n","        return input_dict\n","    with torch.no_grad():\n","        names = []\n","        values = []\n","        # sort the keys so that they are consistent across processes\n","        for k in sorted(input_dict.keys()):\n","            names.append(k)\n","            values.append(input_dict[k])\n","        values = torch.stack(values, dim=0)\n","        dist.all_reduce(values)\n","        if average:\n","            values /= world_size\n","        reduced_dict = {k: v for k, v in zip(names, values)}\n","    return reduced_dict\n","\n","\n","class MetricLogger(object):\n","    def __init__(self, delimiter=\"\\t\"):\n","        self.meters = defaultdict(SmoothedValue)\n","        self.delimiter = delimiter\n","\n","    def update(self, **kwargs):\n","        for k, v in kwargs.items():\n","            if isinstance(v, torch.Tensor):\n","                v = v.item()\n","            assert isinstance(v, (float, int))\n","            self.meters[k].update(v)\n","\n","    def __getattr__(self, attr):\n","        if attr in self.meters:\n","            return self.meters[attr]\n","        if attr in self.__dict__:\n","            return self.__dict__[attr]\n","        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n","            type(self).__name__, attr))\n","\n","    def __str__(self):\n","        loss_str = []\n","        for name, meter in self.meters.items():\n","            loss_str.append(\n","                \"{}: {}\".format(name, str(meter))\n","            )\n","        return self.delimiter.join(loss_str)\n","\n","    def synchronize_between_processes(self):\n","        for meter in self.meters.values():\n","            meter.synchronize_between_processes()\n","\n","    def add_meter(self, name, meter):\n","        self.meters[name] = meter\n","\n","    def log_every(self, iterable, print_freq, header=None):\n","        i = 0\n","        if not header:\n","            header = ''\n","        start_time = time.time()\n","        end = time.time()\n","        iter_time = SmoothedValue(fmt='{avg:.4f}')\n","        data_time = SmoothedValue(fmt='{avg:.4f}')\n","        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n","        if torch.cuda.is_available():\n","            log_msg = self.delimiter.join([\n","                header,\n","                '[{0' + space_fmt + '}/{1}]',\n","                'eta: {eta}',\n","                '{meters}',\n","                'time: {time}',\n","                'data: {data}',\n","                'max mem: {memory:.0f}'\n","            ])\n","        else:\n","            log_msg = self.delimiter.join([\n","                header,\n","                '[{0' + space_fmt + '}/{1}]',\n","                'eta: {eta}',\n","                '{meters}',\n","                'time: {time}',\n","                'data: {data}'\n","            ])\n","        MB = 1024.0 * 1024.0\n","        for obj in iterable:\n","            data_time.update(time.time() - end)\n","            yield obj\n","            iter_time.update(time.time() - end)\n","            if i % print_freq == 0 or i == len(iterable) - 1:\n","                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n","                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n","                if torch.cuda.is_available():\n","                    print(log_msg.format(\n","                        i, len(iterable), eta=eta_string,\n","                        meters=str(self),\n","                        time=str(iter_time), data=str(data_time),\n","                        memory=torch.cuda.max_memory_allocated() / MB))\n","                else:\n","                    print(log_msg.format(\n","                        i, len(iterable), eta=eta_string,\n","                        meters=str(self),\n","                        time=str(iter_time), data=str(data_time)))\n","            i += 1\n","            end = time.time()\n","        total_time = time.time() - start_time\n","        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n","        print('{} Total time: {} ({:.4f} s / it)'.format(\n","            header, total_time_str, total_time / len(iterable)))\n","\n","\n","def get_sha():\n","    cwd = os.path.dirname(os.path.abspath(__file__))\n","\n","    def _run(command):\n","        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n","    sha = 'N/A'\n","    diff = \"clean\"\n","    branch = 'N/A'\n","    try:\n","        sha = _run(['git', 'rev-parse', 'HEAD'])\n","        subprocess.check_output(['git', 'diff'], cwd=cwd)\n","        diff = _run(['git', 'diff-index', 'HEAD'])\n","        diff = \"has uncommited changes\" if diff else \"clean\"\n","        branch = _run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n","    except Exception:\n","        pass\n","    message = f\"sha: {sha}, status: {diff}, branch: {branch}\"\n","    return message\n","\n","\n","def collate_fn(batch):\n","    batch = list(zip(*batch))\n","    batch[0] = nested_tensor_from_tensor_list(batch[0])\n","    return tuple(batch)\n","\n","def collate_fn_crowd(batch):\n","    # re-organize the batch\n","    batch_new = []\n","    for b in batch:\n","        imgs, points = b\n","        if imgs.ndim == 3:\n","            imgs = imgs.unsqueeze(0)\n","        for i in range(len(imgs)):\n","            batch_new.append((imgs[i, :, :, :], points[i]))\n","    batch = batch_new\n","    batch = list(zip(*batch))\n","    batch[0] = nested_tensor_from_tensor_list(batch[0])\n","    return tuple(batch)\n","\n","\n","def _max_by_axis(the_list):\n","    # type: (List[List[int]]) -> List[int]\n","    maxes = the_list[0]\n","    for sublist in the_list[1:]:\n","        for index, item in enumerate(sublist):\n","            maxes[index] = max(maxes[index], item)\n","    return maxes\n","\n","def _max_by_axis_pad(the_list):\n","    # type: (List[List[int]]) -> List[int]\n","    maxes = the_list[0]\n","    for sublist in the_list[1:]:\n","        for index, item in enumerate(sublist):\n","            maxes[index] = max(maxes[index], item)\n","\n","    block = 128\n","\n","    for i in range(2):\n","        maxes[i+1] = ((maxes[i+1] - 1) // block + 1) * block\n","    return maxes\n","\n","\n","def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n","    # TODO make this more general\n","    if tensor_list[0].ndim == 3:\n","\n","        # TODO make it support different-sized images\n","        max_size = _max_by_axis_pad([list(img.shape) for img in tensor_list])\n","        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n","        batch_shape = [len(tensor_list)] + max_size\n","        b, c, h, w = batch_shape\n","        dtype = tensor_list[0].dtype\n","        device = tensor_list[0].device\n","        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n","        for img, pad_img in zip(tensor_list, tensor):\n","            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n","    else:\n","        raise ValueError('not supported')\n","    return tensor\n","\n","class NestedTensor(object):\n","    def __init__(self, tensors, mask: Optional[Tensor]):\n","        self.tensors = tensors\n","        self.mask = mask\n","\n","    def to(self, device):\n","        # type: (Device) -> NestedTensor # noqa\n","        cast_tensor = self.tensors.to(device)\n","        mask = self.mask\n","        if mask is not None:\n","            assert mask is not None\n","            cast_mask = mask.to(device)\n","        else:\n","            cast_mask = None\n","        return NestedTensor(cast_tensor, cast_mask)\n","\n","    def decompose(self):\n","        return self.tensors, self.mask\n","\n","    def __repr__(self):\n","        return str(self.tensors)\n","\n","\n","def setup_for_distributed(is_master):\n","    \"\"\"\n","    This function disables printing when not in master process\n","    \"\"\"\n","    import builtins as __builtin__\n","    builtin_print = __builtin__.print\n","\n","    def print(*args, **kwargs):\n","        force = kwargs.pop('force', False)\n","        if is_master or force:\n","            builtin_print(*args, **kwargs)\n","\n","    __builtin__.print = print\n","\n","\n","def is_dist_avail_and_initialized():\n","    if not dist.is_available():\n","        return False\n","    if not dist.is_initialized():\n","        return False\n","    return True\n","\n","\n","def get_world_size():\n","    if not is_dist_avail_and_initialized():\n","        return 1\n","    return dist.get_world_size()\n","\n","\n","def get_rank():\n","    if not is_dist_avail_and_initialized():\n","        return 0\n","    return dist.get_rank()\n","\n","\n","def is_main_process():\n","    return get_rank() == 0\n","\n","\n","def save_on_master(*args, **kwargs):\n","    if is_main_process():\n","        torch.save(*args, **kwargs)\n","\n","\n","def init_distributed_mode(args):\n","    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n","        args.rank = int(os.environ[\"RANK\"])\n","        args.world_size = int(os.environ['WORLD_SIZE'])\n","        args.gpu = int(os.environ['LOCAL_RANK'])\n","    elif 'SLURM_PROCID' in os.environ:\n","        args.rank = int(os.environ['SLURM_PROCID'])\n","        args.gpu = args.rank % torch.cuda.device_count()\n","    else:\n","        print('Not using distributed mode')\n","        args.distributed = False\n","        return\n","\n","    args.distributed = True\n","\n","    torch.cuda.set_device(args.gpu)\n","    args.dist_backend = 'nccl'\n","    print('| distributed init (rank {}): {}'.format(\n","        args.rank, args.dist_url), flush=True)\n","    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n","                                         world_size=args.world_size, rank=args.rank)\n","    torch.distributed.barrier()\n","    setup_for_distributed(args.rank == 0)\n","\n","\n","@torch.no_grad()\n","def accuracy(output, target, topk=(1,)):\n","    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n","    if target.numel() == 0:\n","        return [torch.zeros([], device=output.device)]\n","    maxk = max(topk)\n","    batch_size = target.size(0)\n","\n","    _, pred = output.topk(maxk, 1, True, True)\n","    pred = pred.t()\n","    correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","    res = []\n","    for k in topk:\n","        correct_k = correct[:k].view(-1).float().sum(0)\n","        res.append(correct_k.mul_(100.0 / batch_size))\n","    return res\n","\n","\n","def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n","    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n","    \"\"\"\n","    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n","    This will eventually be supported natively by PyTorch, and this\n","    class can go away.\n","    \"\"\"\n","    if float(torchvision.__version__[:3]) < 0.7:\n","        if input.numel() > 0:\n","            return torch.nn.functional.interpolate(\n","                input, size, scale_factor, mode, align_corners\n","            )\n","\n","        output_shape = _output_size(2, input, size, scale_factor)\n","        output_shape = list(input.shape[:-2]) + list(output_shape)\n","        return _new_empty_tensor(input, output_shape)\n","    else:\n","        return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)\n","\n","\n","class FocalLoss(nn.Module):\n","    r\"\"\"\n","        This criterion is a implemenation of Focal Loss, which is proposed in\n","        Focal Loss for Dense Object Detection.\n","            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n","        The losses are averaged across observations for each minibatch.\n","        Args:\n","            alpha(1D Tensor, Variable) : the scalar factor for this criterion\n","            gamma(float, double) : gamma > 0; reduces the relative loss for well-classiﬁed examples (p > .5),\n","                                   putting more focus on hard, misclassiﬁed examples\n","            size_average(bool): By default, the losses are averaged over observations for each minibatch.\n","                                However, if the field size_average is set to False, the losses are\n","                                instead summed for each minibatch.\n","    \"\"\"\n","    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n","        super(FocalLoss, self).__init__()\n","        if alpha is None:\n","            self.alpha = Variable(torch.ones(class_num, 1))\n","        else:\n","            if isinstance(alpha, Variable):\n","                self.alpha = alpha\n","            else:\n","                self.alpha = Variable(alpha)\n","        self.gamma = gamma\n","        self.class_num = class_num\n","        self.size_average = size_average\n","\n","    def forward(self, inputs, targets):\n","        N = inputs.size(0)\n","        C = inputs.size(1)\n","        P = F.softmax(inputs)\n","\n","        class_mask = inputs.data.new(N, C).fill_(0)\n","        class_mask = Variable(class_mask)\n","        ids = targets.view(-1, 1)\n","        class_mask.scatter_(1, ids.data, 1.)\n","\n","        if inputs.is_cuda and not self.alpha.is_cuda:\n","            self.alpha = self.alpha.cuda()\n","        alpha = self.alpha[ids.data.view(-1)]\n","\n","        probs = (P*class_mask).sum(1).view(-1,1)\n","\n","        log_p = probs.log()\n","        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p\n","\n","        if self.size_average:\n","            loss = batch_loss.mean()\n","        else:\n","            loss = batch_loss.sum()\n","        return "],"metadata":{"id":"jNz8iOaxdj6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import models.vgg_ as models\n","import torch\n","import torch.nn as nn\n","\n","\n","__all__ = [\n","    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n","    'vgg19_bn', 'vgg19',\n","]\n","\n","model_urls = {\n","    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n","    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n","    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n","    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n","    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n","    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n","    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n","    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n","}\n","\n","model_paths = {\n","    'vgg16_bn': '/apdcephfs/private_changanwang/checkpoints/vgg16_bn-6c64b313.pth',\n","    'vgg16': '/apdcephfs/private_changanwang/checkpoints/vgg16-397923af.pth',\n","\n","}\n","\n","class VGG(nn.Module):\n","\n","    def __init__(self, features, num_classes=1000, init_weights=True):\n","        super(VGG, self).__init__()\n","        self.features = features\n","        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, num_classes),\n","        )\n","        if init_weights:\n","            self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","\n","def make_layers(cfg, batch_norm=False, sync=False):\n","    layers = []\n","    in_channels = 3\n","    for v in cfg:\n","        if v == 'M':\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","            if batch_norm:\n","                if sync:\n","                    print('use sync backbone')\n","                    layers += [conv2d, nn.SyncBatchNorm(v), nn.ReLU(inplace=True)]\n","                else:\n","                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","            else:\n","                layers += [conv2d, nn.ReLU(inplace=True)]\n","            in_channels = v\n","    return nn.Sequential(*layers)\n","\n","\n","cfgs = {\n","    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","\n","def _vgg(arch, cfg, batch_norm, pretrained, progress, sync=False, **kwargs):\n","    if pretrained:\n","        kwargs['init_weights'] = False\n","    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm, sync=sync), **kwargs)\n","    if pretrained:\n","        state_dict = torch.hub.load_state_dict_from_url(model_urls[arch], progress=True)\n","        model.load_state_dict(state_dict)\n","    return model\n","\n","\n","def vgg16(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"VGG 16-layer model (configuration \"D\")\n","    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _vgg('vgg16', 'D', False, pretrained, progress, **kwargs)\n","\n","\n","def vgg16_bn(pretrained=False, progress=True, sync=False, **kwargs):\n","    r\"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n","    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _vgg('vgg16_bn', 'D', True, pretrained, progress, sync=sync, **kwargs)\n","\n"],"metadata":{"id":"yxAGuGck7PdB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##\n","#from models.backbone import build_backbone\n","from collections import OrderedDict\n","import torch\n","import torch.nn.functional as F\n","import torchvision\n","from torch import nn\n","\n","\n","class BackboneBase_VGG(nn.Module):\n","    def __init__(self, backbone: nn.Module, num_channels: int, name: str, return_interm_layers: bool):\n","        super().__init__()\n","        features = list(backbone.features.children())\n","        if return_interm_layers:\n","            if name == 'vgg16_bn':\n","                self.body1 = nn.Sequential(*features[:13])\n","                self.body2 = nn.Sequential(*features[13:23])\n","                self.body3 = nn.Sequential(*features[23:33])\n","                self.body4 = nn.Sequential(*features[33:43])\n","            else:\n","                self.body1 = nn.Sequential(*features[:9])\n","                self.body2 = nn.Sequential(*features[9:16])\n","                self.body3 = nn.Sequential(*features[16:23])\n","                self.body4 = nn.Sequential(*features[23:30])\n","        else:\n","            if name == 'vgg16_bn':\n","                self.body = nn.Sequential(*features[:44])  # 16x down-sample\n","            elif name == 'vgg16':\n","                self.body = nn.Sequential(*features[:30])  # 16x down-sample\n","        self.num_channels = num_channels\n","        self.return_interm_layers = return_interm_layers\n","\n","    def forward(self, tensor_list):\n","        out = []\n","\n","        if self.return_interm_layers:\n","            xs = tensor_list\n","            for _, layer in enumerate([self.body1, self.body2, self.body3, self.body4]):\n","                xs = layer(xs)\n","                out.append(xs)\n","\n","        else:\n","            xs = self.body(tensor_list)\n","            out.append(xs)\n","        return out\n","\n","\n","class Backbone_VGG(BackboneBase_VGG):\n","    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n","    def __init__(self, name: str, return_interm_layers: bool):\n","        if name == 'vgg16_bn':\n","            backbone = vgg16_bn(pretrained=True)\n","        elif name == 'vgg16':\n","            backbone = vgg16(pretrained=True)\n","        num_channels = 256\n","        super().__init__(backbone, num_channels, name, return_interm_layers)\n","\n","\n","def build_backbone(args):\n","    backbone = Backbone_VGG(args.backbone, True)\n","    return backbone"],"metadata":{"id":"nVI3Wgk86_31"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from scipy.optimize import linear_sum_assignment\n","from torch import nn\n","\n","class HungarianMatcher_Crowd(nn.Module):\n","    \"\"\"This class computes an assignment between the targets and the predictions of the network\n","    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n","    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n","    while the others are un-matched (and thus treated as non-objects).\n","    \"\"\"\n","\n","    def __init__(self, cost_class: float = 1, cost_point: float = 1):\n","        \"\"\"Creates the matcher\n","        Params:\n","            cost_class: This is the relative weight of the foreground object\n","            cost_point: This is the relative weight of the L1 error of the points coordinates in the matching cost\n","        \"\"\"\n","        super().__init__()\n","        self.cost_class = cost_class\n","        self.cost_point = cost_point\n","        assert cost_class != 0 or cost_point != 0, \"all costs cant be 0\"\n","\n","    @torch.no_grad()\n","    def forward(self, outputs, targets):\n","        \"\"\" Performs the matching\n","        Params:\n","            outputs: This is a dict that contains at least these entries:\n","                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n","                 \"points\": Tensor of dim [batch_size, num_queries, 2] with the predicted point coordinates\n","            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n","                 \"labels\": Tensor of dim [num_target_points] (where num_target_points is the number of ground-truth\n","                           objects in the target) containing the class labels\n","                 \"points\": Tensor of dim [num_target_points, 2] containing the target point coordinates\n","        Returns:\n","            A list of size batch_size, containing tuples of (index_i, index_j) where:\n","                - index_i is the indices of the selected predictions (in order)\n","                - index_j is the indices of the corresponding selected targets (in order)\n","            For each batch element, it holds:\n","                len(index_i) = len(index_j) = min(num_queries, num_target_points)\n","        \"\"\"\n","        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n","\n","        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  \n","        out_points = outputs[\"pred_points\"].flatten(0, 1)  \n","\n","        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n","        tgt_points = torch.cat([v[\"point\"] for v in targets])\n","\n","        cost_class = -out_prob[:, tgt_ids]\n","\n","        cost_point = torch.cdist(out_points, tgt_points, p=2)\n","\n","        # Final cost matrix\n","        C = self.cost_point * cost_point + self.cost_class * cost_class\n","        C = C.view(bs, num_queries, -1).cpu()\n","\n","        sizes = [len(v[\"point\"]) for v in targets]\n","        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n","        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n","\n","\n","def build_matcher_crowd(args):\n","    return HungarianMatcher_Crowd(cost_class=args.set_cost_class, cost_point=args.set_cost_point)"],"metadata":{"id":"fIxlY4qD_Ea7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3M_OTlkVPO5"},"outputs":[],"source":["# build model p2pNet.py\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","\n","import numpy as np\n","import time\n","\n","# the network frmawork of the regression branch\n","class RegressionModel(nn.Module):\n","    def __init__(self, num_features_in, num_anchor_points=4, feature_size=256):\n","        super(RegressionModel, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n","        self.act1 = nn.ReLU()\n","\n","        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n","        self.act2 = nn.ReLU()\n","\n","        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n","        self.act3 = nn.ReLU()\n","\n","        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n","        self.act4 = nn.ReLU()\n","\n","        self.output = nn.Conv2d(feature_size, num_anchor_points * 2, kernel_size=3, padding=1) # one point has two coordinates \n","    # sub-branch forward\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.act1(out)\n","\n","        out = self.conv2(out)\n","        out = self.act2(out)\n","\n","        out = self.output(out)\n","\n","        out = out.permute(0, 2, 3, 1)\n","\n","        return out.contiguous().view(out.shape[0], -1, 2)\n","\n","# the network frmawork of the classification branch\n","class ClassificationModel(nn.Module):\n","    def __init__(self, num_features_in, num_anchor_points=4, num_classes=80, prior=0.01, feature_size=256):\n","        super(ClassificationModel, self).__init__()\n","\n","        self.num_classes = num_classes\n","        self.num_anchor_points = num_anchor_points\n","\n","        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n","        self.act1 = nn.ReLU()\n","\n","        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n","        self.act2 = nn.ReLU()\n","\n","        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n","        self.act3 = nn.ReLU()\n","\n","        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n","        self.act4 = nn.ReLU()\n","\n","        self.output = nn.Conv2d(feature_size, num_anchor_points * num_classes, kernel_size=3, padding=1) # one classes, only positives \n","        self.output_act = nn.Sigmoid()\n","    # sub-branch forward\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.act1(out)\n","\n","        out = self.conv2(out)\n","        out = self.act2(out)\n","\n","        out = self.output(out)\n","\n","        out1 = out.permute(0, 2, 3, 1)\n","\n","        batch_size, width, height, _ = out1.shape\n","\n","        out2 = out1.view(batch_size, width, height, self.num_anchor_points, self.num_classes)\n","\n","        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n","\n","# generate the reference points in grid layout\n","def generate_anchor_points(stride=8, row=3, line=3):\n","    row_step = stride / row\n","    line_step = stride / line\n","\n","    shift_x = (np.arange(1, line + 1) - 0.5) * line_step - stride / 2\n","    shift_y = (np.arange(1, row + 1) - 0.5) * row_step - stride / 2\n","\n","    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n","\n","    anchor_points = np.vstack((\n","        shift_x.ravel(), shift_y.ravel()\n","    )).transpose()\n","\n","    return anchor_points\n","# shift the meta-anchor to get an acnhor points\n","def shift(shape, stride, anchor_points):\n","    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n","    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n","\n","    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n","\n","    shifts = np.vstack((\n","        shift_x.ravel(), shift_y.ravel()\n","    )).transpose()\n","\n","    A = anchor_points.shape[0]\n","    K = shifts.shape[0]\n","    all_anchor_points = (anchor_points.reshape((1, A, 2)) + shifts.reshape((1, K, 2)).transpose((1, 0, 2)))\n","    all_anchor_points = all_anchor_points.reshape((K * A, 2))\n","\n","    return all_anchor_points\n","\n","# this class generate all reference points on all pyramid levels\n","class AnchorPoints(nn.Module):\n","    def __init__(self, pyramid_levels=None, strides=None, row=3, line=3):\n","        super(AnchorPoints, self).__init__()\n","\n","        if pyramid_levels is None:\n","            self.pyramid_levels = [3, 4, 5, 6, 7]\n","        else:\n","            self.pyramid_levels = pyramid_levels\n","\n","        if strides is None:\n","            self.strides = [2 ** x for x in self.pyramid_levels]\n","\n","        self.row = row\n","        self.line = line\n","\n","    def forward(self, image):\n","        image_shape = image.shape[2:]\n","        image_shape = np.array(image_shape)\n","        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels] # calcualtes the output size of the model (image of 128*128 to feature map of 16*16)\n","\n","        all_anchor_points = np.zeros((0, 2)).astype(np.float32)\n","        # get reference points for each level\n","        for idx, p in enumerate(self.pyramid_levels):\n","            anchor_points = generate_anchor_points(2**p, row=self.row, line=self.line)\n","            shifted_anchor_points = shift(image_shapes[idx], self.strides[idx], anchor_points)\n","            all_anchor_points = np.append(all_anchor_points, shifted_anchor_points, axis=0)\n","\n","        all_anchor_points = np.expand_dims(all_anchor_points, axis=0)\n","        # send reference points to device\n","        if torch.cuda.is_available():\n","            return torch.from_numpy(all_anchor_points.astype(np.float32)).cuda()\n","        else:\n","            return torch.from_numpy(all_anchor_points.astype(np.float32))\n","# bacauce of the two big size of these pods, put another layer to further lower the spatial dimension of the input image\n","class vgg_L32(nn.Module):\n","    def __init__(self):\n","        super(vgg_L32, self).__init__()\n","        self.conv2d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.bn = nn.BatchNorm2d()\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpol = nn.MaxPool2d(kernel_size=2, stride=2)\n","    def forward(self, x):\n","        x = self.conv2d(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        x = self.maxpol(x)\n","        return x\n","\n","##\n","class Decoder(nn.Module):\n","    def __init__(self, C3_size, C4_size, C5_size, feature_size=16):\n","        super(Decoder, self).__init__()\n","        # upsample C5 to get P5 from the FPN paper\n","        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n","        self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n","        self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n","\n","        # add P5 elementwise to C4\n","        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n","        self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n","        self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n","\n","        # add P4 elementwise to C3\n","        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n","        self.P3_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n","        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n","\n","\n","    def forward(self, inputs):\n","        C3, C4, C5 = inputs\n","\n","        P5_x = self.P5_1(C5)\n","        P5_upsampled_x = self.P5_upsampled(P5_x)\n","        P5_x = self.P5_2(P5_x)\n","\n","        P4_x = self.P4_1(C4)\n","        P4_x = P5_upsampled_x + P4_x\n","        P4_upsampled_x = self.P4_upsampled(P4_x)\n","        P4_x = self.P4_2(P4_x)\n","\n","        P3_x = self.P3_1(C3)\n","        P3_x = P3_x + P4_upsampled_x\n","        P3_x = self.P3_2(P3_x)\n","\n","        return [P3_x, P4_x, P5_x]\n","##\n","# the defenition of the P2PNet model\n","class P2PNet(nn.Module):\n","    def __init__(self, backbone, row=2, line=2):\n","        super().__init__()\n","        self.backbone = backbone\n","        self.num_classes = 2\n","        # the number of all anchor points\n","        num_anchor_points = row * line\n","\n","        self.regression = RegressionModel(num_features_in=16, num_anchor_points=num_anchor_points)\n","        self.classification = ClassificationModel(num_features_in=16, \\\n","                                            num_classes=self.num_classes, \\\n","                                            num_anchor_points=num_anchor_points)\n","\n","        self.anchor_points = AnchorPoints(pyramid_levels=[2,], row=row, line=line) # remember to change pyramid level when you change feature input\n","\n","        self.fpn = Decoder(256, 512, 512)\n","\n","    def forward(self, samples: NestedTensor):\n","        # get the backbone features\n","        features = self.backbone(samples)\n","        # forward the feature pyramid\n","        features_fpn = self.fpn([features[1], features[2], features[3]]) # contains 4 features now\n","\n","        batch_size = features[1].shape[0]\n","\n","        # run the regression and classification branch\n","        regression = self.regression(features_fpn[0]) * 100 # 8x\n","\n","        classification = self.classification(features_fpn[0])\n","        anchor_points = self.anchor_points(samples).repeat(batch_size, 1, 1)\n","        # decode the points as prediction\n","        output_coord = regression + anchor_points\n","        output_class = classification\n","        out = {'pred_logits': output_class, 'pred_points': output_coord}\n","       \n","        return out\n","\n","class SetCriterion_Crowd(nn.Module):\n","\n","    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n","        \"\"\" Create the criterion.\n","        Parameters:\n","            num_classes: number of object categories, omitting the special no-object category\n","            matcher: module able to compute a matching between targets and proposals\n","            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n","            eos_coef: relative classification weight applied to the no-object category\n","            losses: list of all the losses to be applied. See get_loss for list of available losses.\n","        \"\"\"\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.matcher = matcher\n","        self.weight_dict = weight_dict\n","        self.eos_coef = eos_coef\n","        self.losses = losses\n","        empty_weight = torch.ones(self.num_classes + 1)\n","        empty_weight[0] = self.eos_coef\n","        self.register_buffer('empty_weight', empty_weight)\n","\n","    def loss_labels(self, outputs, targets, indices, num_points):\n","        \"\"\"Classification loss (NLL)\n","        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n","        \"\"\"\n","        assert 'pred_logits' in outputs\n","        src_logits = outputs['pred_logits']\n","\n","        idx = self._get_src_permutation_idx(indices)\n","        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n","        target_classes = torch.full(src_logits.shape[:2], 0,\n","                                    dtype=torch.int64, device=src_logits.device)\n","        target_classes[idx] = target_classes_o\n","\n","        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n","        losses = {'loss_ce': loss_ce}\n","\n","        return losses\n","\n","    def loss_points(self, outputs, targets, indices, num_points):\n","\n","        assert 'pred_points' in outputs\n","        idx = self._get_src_permutation_idx(indices)\n","        src_points = outputs['pred_points'][idx]\n","        target_points = torch.cat([t['point'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n","        loss_bbox = F.mse_loss(src_points, target_points, reduction='none')\n","\n","        losses = {}\n","        losses['loss_points'] = loss_bbox.sum() / num_points\n","\n","        return losses\n","\n","    def _get_src_permutation_idx(self, indices):\n","        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n","        src_idx = torch.cat([src for (src, _) in indices])\n","        return batch_idx, src_idx\n","\n","    def _get_tgt_permutation_idx(self, indices):\n","        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n","        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n","        return batch_idx, tgt_idx\n","\n","    def get_loss(self, loss, outputs, targets, indices, num_points, **kwargs):\n","        loss_map = {\n","            'labels': self.loss_labels,\n","            'points': self.loss_points,\n","        }\n","        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n","        return loss_map[loss](outputs, targets, indices, num_points, **kwargs)\n","\n","    def forward(self, outputs, targets):\n","        \"\"\" This performs the loss computation.\n","        Parameters:\n","             outputs: dict of tensors, see the output specification of the model for the format\n","             targets: list of dicts, such that len(targets) == batch_size.\n","                      The expected keys in each dict depends on the losses applied, see each loss' doc\n","        \"\"\"\n","        output1 = {'pred_logits': outputs['pred_logits'], 'pred_points': outputs['pred_points']}\n","        indices1 = self.matcher(output1, targets)\n","        #\n","\n","        num_points = sum(len(t[\"labels\"]) for t in targets)\n","        num_points = torch.as_tensor([num_points], dtype=torch.float, device=next(iter(output1.values())).device)\n","        if is_dist_avail_and_initialized():\n","            torch.distributed.all_reduce(num_points)\n","        # \n","        num_boxes = torch.clamp(num_points / get_world_size(), min=1).item()\n","\n","        losses = {}\n","        for loss in self.losses:\n","            losses.update(self.get_loss(loss, output1, targets, indices1, num_boxes))\n","\n","        return losses\n","\n","# create the P2PNet model\n","def build(args, training):\n","    # treats persons as a single class\n","    num_classes = 1\n","\n","    backbone = build_backbone(args)\n","    model = P2PNet(backbone, args.row, args.line)\n","    if not training: \n","        return model\n","\n","    weight_dict = {'loss_ce': 1, 'loss_points': args.point_loss_coef}\n","    losses = ['labels', 'points']  \n","    matcher = build_matcher_crowd(args)\n","    criterion = SetCriterion_Crowd(num_classes, \\\n","                                matcher=matcher, weight_dict=weight_dict, \\\n","                                eos_coef=args.eos_coef, losses=losses)\n","\n","    return model, criterion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ay_LGQKRsUmX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zllbyl5BOlQj"},"outputs":[],"source":["\n","def get_arguments():\n","    \"\"\"Parse all the arguments provided from the CLI.\n","    Returns:\n","      A list of parsed arguments.\n","    \"\"\"\n","    parser = argparse.ArgumentParser(description=\"Object Counting Framework\")\n","    # constant\n","    parser.add_argument('--lr', default=1e-4, type=float)\n","    parser.add_argument('--lr_fpn', default=1e-5, type=float)\n","    parser.add_argument('--batch_size', default=1, type=int)\n","    parser.add_argument('--weight_decay', default=1e-4, type=float)\n","    parser.add_argument('--epochs', default=3500, type=int)\n","    parser.add_argument('--lr_drop', default=1000, type=int)\n","    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n","                        help='gradient clipping max norm')\n","\n","    # Model parameters\n","    parser.add_argument('--frozen_weights', type=str, default=None,\n","                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n","\n","    # * Backbone\n","    parser.add_argument('--backbone', default='vgg16', type=str,\n","                        help=\"Name of the convolutional backbone to use\")\n","    #\n","    # * Matcher is a Hungarian strategy, minimizing the cost of proposed points and gt points\n","    # cost matrix D = set_cost_point * L2 distance between proposed points and gt points - set_cost_class * confidence score of proposed points \n","    # (if Ppoint and GTpoint are close, the cost D will be small !!)\n","    parser.add_argument('--set_cost_class', default=1, type=float,\n","                        help=\"Class coefficient in the matching cost\")\n","\n","    parser.add_argument('--set_cost_point', default=0.5, type=float,\n","                        help=\"L1 point coefficient in the matching cost\")\n","\n","    # * Loss coefficients\n","    # assuming the weight of classification loss is constant, eg., 1, \n","    # the point loss coefficient controls how much the point distance loss contribute to the final loss\n","    parser.add_argument('--point_loss_coef', default=0.0002, type=float) # default = 0.0002 # 0.5\n","    # the final classification loss = -(the sum of positive confidence score + eos_coef * the sum of negative confidence score)/M, M is total proposed points  \n","    parser.add_argument('--eos_coef', default=0.05, type=float, # 0.05\n","                        help=\"Relative classification weight of the no-object class\") # default = 0.5\n","    \n","    # a threshold during evaluation for counting and visualization\n","    parser.add_argument('--threshold', default=0.5, type=float,\n","                        help=\"threshold in evalluation: evaluate_crowd_no_overlap\")\n","    parser.add_argument('--row', default=2, type=int,\n","                        help=\"row number of anchor points\")\n","    parser.add_argument('--line', default=2, type=int,\n","                        help=\"line number of anchor points\")\n","\n","    # dataset parameters\n","    parser.add_argument('--dataset_file', default='SHHA')\n","    parser.add_argument('--data_root', default='/content/drive/My Drive/soypod_crop_counting/',\n","                        help='path where the dataset is')\n","    \n","    parser.add_argument('--output_dir', default='/content/drive/My Drive/P2PNet-Soy/log_originalP2PNet_PlusKDtree',\n","                        help='path where to save, empty for no saving')\n","    parser.add_argument('--checkpoints_dir', default='/content/drive/My Drive/P2PNet-Soy/ckpt_originalP2PNet_PlusKDtree_01',\n","                        help='path where to save checkpoints, empty for no saving') #ckpt_5n was not bad, default 2 X 2\n","    parser.add_argument('--tensorboard_dir', default='/content/drive/My Drive/P2PNet-Soy/runs_originalP2PNet_PlusKDtree',\n","                        help='path where to save, empty for no saving')\n","\n","    parser.add_argument('--seed', default=42, type=int)\n","    parser.add_argument('--resume', default='', help='resume from checkpoint')\n","    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n","                        help='start epoch')\n","    parser.add_argument('--eval', action='store_true')\n","    parser.add_argument('--num_workers', default=1, type=int)\n","    parser.add_argument('--eval_freq', default=2, type=int,\n","                        help='frequency of evaluation, default setting is evaluating in every 5 epoch')\n","    parser.add_argument('--gpu_id', default=0, type=int, help='the gpu used for training')\n","    #\n","    opt = parser.parse_known_args()[0] \n","    return opt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gl6zkwnxkqJs"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzU3pfq1Js4r","outputId":"5040a07f-b271-49fb-b945-30aa8a6aa087","executionInfo":{"status":"ok","timestamp":1667900183886,"user_tz":-540,"elapsed":2353,"user":{"displayName":"jiangsan zhao","userId":"15833650277394623841"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(backbone='vgg16', batch_size=1, checkpoints_dir='/content/drive/My Drive/CrowdCounting-P2PNet/ckpt_p2pnet_original_trying', clip_max_norm=0.1, data_root='/content/drive/My Drive/soypod_crop_counting/', dataset_file='SHHA', eos_coef=0.05, epochs=3500, eval=False, eval_freq=2, frozen_weights=None, gpu_id=0, line=2, lr=0.0001, lr_drop=1000, lr_fpn=1e-05, num_workers=1, output_dir='/content/drive/My Drive/CrowdCounting-P2PNet/log_p2pnet_original_trying', point_loss_coef=0.0002, resume='/content/drive/My Drive/CrowdCounting-P2PNet/ckpt_p2pnet_original_trying/best_mae.pth', row=2, seed=42, set_cost_class=1, set_cost_point=0.5, start_epoch=0, tensorboard_dir='/content/drive/My Drive/CrowdCounting-P2PNet/runs_p2pnet_original_trying', threshold=0.5, vis_dir='/content/drive/My Drive/CrowdCounting-P2PNet/vis_p2pnet_original_trying', weight_decay=0.0001)\n","number of params: 18393776\n","Start training\n"]}],"source":["args = get_arguments()\n","#\n","#args.frozen_weights = \"yes\"\n","#\n","args.resume = \"/content/drive/My Drive/P2PNet-Soy/ckpt_originalP2PNet_PlusKDtree/best_mae.pth\"\n","#\n","args.vis_dir = \"/content/drive/My Drive/P2PNet-Soy/vis_originalP2PNet_PlusKDtree\"\n","if not os.path.exists(args.vis_dir):\n","    os.makedirs(args.vis_dir)\n","##\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{}'.format(args.gpu_id)\n","# create the logging file\n","if not os.path.exists(args.output_dir):\n","    os.makedirs(args.output_dir)\n","run_log_name = os.path.join(args.output_dir, 'run_log.txt')\n","with open(run_log_name, \"w\") as log_file:\n","    log_file.write('Eval Log %s\\n' % time.strftime(\"%c\"))\n","\n","#if args.frozen_weights is not None:\n","#    assert args.masks, \"Frozen training is meant for segmentation only\"\n","# backup the arguments\n","print(args)\n","with open(run_log_name, \"a\") as log_file:\n","    log_file.write(\"{}\".format(args))\n","device = torch.device('cuda')\n","# fix the seed for reproducibility\n","seed = args.seed + utils.get_rank()\n","seed = args.seed\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","# get the P2PNet model\n","model, criterion = build(args, training=True)\n","\n","# move to GPU\n","model.to(device)\n","criterion.to(device)\n","\n","model_without_ddp = model\n","\n","n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print('number of params:', n_parameters)\n","# use different optimation params for different parts of the model\n","param_dicts = [\n","    {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"fpn\" not in n and p.requires_grad]},\n","    {\n","        \"params\": [p for n, p in model_without_ddp.named_parameters() if \"fpn\" in n and p.requires_grad],\n","        \"lr\": args.lr_fpn,\n","    },\n","]\n","# Adam is used by default\n","optimizer = torch.optim.Adam(param_dicts, lr=args.lr)\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n","# create the training and valiation set\n","train_set, val_set = loading_data(args.data_root)\n","# create the sampler used during training\n","sampler_train = torch.utils.data.RandomSampler(train_set)\n","sampler_val = torch.utils.data.SequentialSampler(val_set)\n","\n","batch_sampler_train = torch.utils.data.BatchSampler(\n","    sampler_train, args.batch_size, drop_last=True)\n","# the dataloader for training\n","data_loader_train = DataLoader(train_set, batch_sampler=batch_sampler_train,\n","                                collate_fn=collate_fn_crowd, num_workers=args.num_workers)\n","\n","data_loader_val = DataLoader(val_set, 1, sampler=sampler_val,\n","                                drop_last=False, collate_fn=collate_fn_crowd, num_workers=args.num_workers)\n","\n","if args.frozen_weights is not None:\n","    checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n","    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n","# resume the weights and training state if exists\n","if args.resume:\n","    checkpoint = torch.load(args.resume, map_location='cpu')\n","    model_without_ddp.load_state_dict(checkpoint['model'])\n","    args.start_epoch = checkpoint['epoch']\n","    new_start = 1\n","    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n","else:\n","    new_start = 0\n","print(\"Start training\")\n","start_time = time.time()\n","# save the performance during the training\n","mae = []\n","mse = []\n","epoch_save = []\n","# the logger writer\n","writer = SummaryWriter(args.tensorboard_dir)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"EqS-mNzcKpMV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AydizQosJs-B"},"outputs":[],"source":["# remember whether to involve post-processing or not !!!!!\n","\n","# threshold for evaluation\n","threshold = args.threshold\n","# \n","# save latest weights every epoch\n","if not os.path.exists(args.checkpoints_dir):\n","    os.makedirs(args.checkpoints_dir)\n","#\n","step = 0\n","# training starts here\n","for epoch in range(args.start_epoch, args.epochs):\n","    # always run evaluation first\n","    if (epoch +2) % args.eval_freq == 0 or new_start: # and epoch != 0\n","        # change the status right after the first iteration\n","        new_start = 0\n","        #\n","        t1 = time.time()\n","        result = evaluate_crowd_no_overlap(model, data_loader_val, device, epoch, threshold, args.vis_dir)\n","        t2 = time.time()\n","\n","        mae.append(result[0])\n","        mse.append(result[1])\n","        epoch_save.append(epoch)\n","        #\n","        epoch_save_m = np.array(epoch_save)[mae == np.min(mae)][0]\n","        # print the evaluation results\n","        print('=======================================test=======================================')\n","        print(\"mae:\", result[0], \"mse:\", result[1], \"time:\", t2 - t1, \"best mae:\", np.min(mae), \"at epoch: {}\".format(epoch_save_m) )\n","        with open(run_log_name, \"a\") as log_file:\n","            log_file.write(\"mae:{}, mse:{}, time:{}, best mae:{}\".format(result[0], \n","                            result[1], t2 - t1, np.min(mae)))\n","        print('=======================================test=======================================')\n","        # recored the evaluation results\n","        if writer is not None:\n","            with open(run_log_name, \"a\") as log_file:\n","                log_file.write(\"metric/mae@{}: {}\".format(step, result[0]))\n","                log_file.write(\"metric/mse@{}: {}\".format(step, result[1]))\n","            writer.add_scalar('metric/mae', result[0], step)\n","            writer.add_scalar('metric/mse', result[1], step)\n","            step += 1\n","\n","        # save the best model since begining\n","        if abs(np.min(mae) - result[0]) < 0.01:\n","            checkpoint_best_path = os.path.join(args.checkpoints_dir, 'best_mae.pth')\n","            torch.save({\n","                'model': model_without_ddp.state_dict(),\n","            }, checkpoint_best_path)\n","    ##\n","    # start training from here\n","    t1 = time.time()\n","    stat = train_one_epoch(model, criterion, data_loader_train, optimizer, device, epoch, args.clip_max_norm)\n","\n","    # record the training states after every epoch\n","    if writer is not None:\n","        with open(run_log_name, \"a\") as log_file:\n","            log_file.write(\"loss/loss@{}: {}\".format(epoch, stat['loss']))\n","            log_file.write(\"loss/loss_ce@{}: {}\".format(epoch, stat['loss_ce']))\n","            \n","        writer.add_scalar('loss/loss', stat['loss'], epoch)\n","        writer.add_scalar('loss/loss_ce', stat['loss_ce'], epoch)\n","\n","    t2 = time.time()\n","    print('[ep %d][lr %.7f][%.2fs]' % \\\n","            (epoch, optimizer.param_groups[0]['lr'], t2 - t1))\n","    with open(run_log_name, \"a\") as log_file:\n","        log_file.write('[ep %d][lr %.7f][%.2fs]' % (epoch, optimizer.param_groups[0]['lr'], t2 - t1))\n","    # change lr according to the scheduler\n","    lr_scheduler.step()\n","    # save latest weights every epoch\n","    if not os.path.exists(args.checkpoints_dir):\n","        os.makedirs(args.checkpoints_dir)\n","    checkpoint_latest_path = os.path.join(args.checkpoints_dir, 'latest.pth')\n","    torch.save({\n","        'model': model_without_ddp.state_dict(),\n","    }, checkpoint_latest_path)\n","    \n","    if epoch % 150 == 0 and epoch != 0:\n","        clear_output()\n","# total time for training\n","total_time = time.time() - start_time\n","total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n","print('Training time {}'.format(total_time_str))\n","# ended in 241 epochs (first round)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmJDLCM9ArSQ"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"qQH0ijpi6_hJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cdKRXL-66_j0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Inference"],"metadata":{"id":"kWcHjadN7B1Q"}},{"cell_type":"code","source":["# create the P2PNet model\n","def build_eval(args):\n","    backbone = build_backbone(args)\n","    model = P2PNet(backbone, args.row, args.line)\n","    return model"],"metadata":{"id":"Zx27Szgu6_mH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## for p2pnet original model\n","def get_arguments():\n","    parser = argparse.ArgumentParser(description=\"Object Counting Framework\")\n","    # * Backbone\n","    parser.add_argument('--backbone', default='vgg16_bn', type=str,\n","                        help=\"Name of the convolutional backbone to use\")\n","    #\n","    # a threshold during evaluation for counting and visualization\n","    parser.add_argument('--threshold', default=0.5, type=float,\n","                        help=\"threshold in evalluation: evaluate_crowd_no_overlap\")\n","    parser.add_argument('--row', default=2, type=int,\n","                        help=\"row number of anchor points\")\n","    parser.add_argument('--line', default=2, type=int,\n","                        help=\"line number of anchor points\")\n","    parser.add_argument('--data_root', default='/content/drive/My Drive/soypod_crop_counting/',\n","                        help='path where the dataset is')\n","    parser.add_argument('--seed', default=42, type=int)\n","    parser.add_argument('--resume', default='', help='resume from checkpoint')\n","    parser.add_argument('--eval', action='store_true')\n","    parser.add_argument('--num_workers', default=1, type=int)\n","    parser.add_argument('--gpu_id', default=0, type=int, help='the gpu used for training')\n","    #\n","    opt = parser.parse_known_args()[0] #if known else parser.parse_args()\n","    return opt"],"metadata":{"id":"IBN1jkhp6_oa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = get_arguments()\n","# specify the directories to model weights and prediction output\n","args.resume = \"/content/drive/My Drive/P2PNet-Soy/ckpt_originalP2PNet_PlusKDtree/best_mae.pth\"\n","args.vis_dir = \"/content/drive/My Drive/P2PNet-Soy/vis_originalP2PNet_PlusKDtree_out\""],"metadata":{"id":"FKjUSViT6_qs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####\n","if not os.path.exists(args.vis_dir):\n","    os.makedirs(args.vis_dir)\n","##\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{}'.format(args.gpu_id)\n","\n","\n","device = torch.device('cuda')\n","# fix the seed for reproducibility\n","seed = args.seed + get_rank()\n","random.seed(seed)\n","# original model\n","model = build_eval(args, training = False)\n","# move to GPU\n","model.to(device)"],"metadata":{"id":"aLDBA24o6_tH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###On images of cropped individual plant"],"metadata":{"id":"EKSa9AVe7OQZ"}},{"cell_type":"code","source":["\n","# threshold for evaluation\n","threshold = 0.5 #args.threshold\n","# to apply post processing or not: if Filter = 10000, the filter is applied\n","\n","#\n","# load trained model\n","if args.resume is not None:\n","    checkpoint = torch.load(args.resume, map_location='cpu')\n","    model.load_state_dict(checkpoint['model'])\n","#\n","model.eval()\n","#\n","step = 0\n","epoch = 0\n","###\n","# create the pre-processing transform\n","transform = standard_transforms.Compose([\n","    standard_transforms.ToTensor(), \n","    standard_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# set your image path here\n","for img_type in [\"b\", \"a\"]: #\n","    print(\"current image folder is {}\".format(img_type))\n","    img_list_path = \"/content/drive/MyDrive/soypod_crop_counting/soypod_crop_counting_{}.txt\".format(img_type)\n","    img_list = [name.split(',') for name in open(img_list_path).read().splitlines()]\n","    #\n","    # save the number of detected pods\n","    csv_name = (args.vis_dir).split(\"/\")[-1]\n","    loss_csv = open(os.path.join(args.vis_dir, '{}_seed_counting_{}.csv'.format(csv_name, img_type)), 'w+')\n","\n","    for img_path_ij, _ in img_list:\n","        print(\"image path = {}\".format(img_path_ij))\n","        #\n","        img_name = img_path_ij.split(\"/\")[-1]\n","        #\n","        img_name_pred_before = img_name.replace(\".png\", '_pred_bf.png')\n","        img_name_pred_after = img_name.replace(\".png\", '_pred_af.png')\n","        # load the images\n","        img_0 = cv2.imread(img_path_ij)\n","        # this is only for drawing points\n","        img_raw = cv2.cvtColor(img_0, cv2.COLOR_BGR2RGB)\n","        # pre-proccessing\n","        img = transform(img_raw)\n","        ##\n","        # round the size\n","        height, width = img_0.shape[:2]\n","        # get the new input image size suitable for VGG16 net\n","        new_width = (width // 128 +1) * 128\n","        new_height = (height // 128 + 1)* 128\n","        ##\n","        img_in = torch.zeros((3, new_height, new_width))\n","        img_in[:,:height,:width] = img\n","        ##\n","        ##\n","        if new_height > 1700:\n","            print(\"Large file\")\n","            # for out out image\n","            img_draw = (np.ones((new_height, new_width, 3))*255).astype(np.uint8)\n","            img_draw[:height, :width,:] = img_raw\n","            #\n","            img_to_draw_before = img_draw.copy() \n","            img_to_draw_after = img_draw.copy() \n","            #\n","            new_width_hf = int(new_width/2)\n","            new_height_hf = int(new_height/2)\n","            for shi in range(1,3):\n","                print(\"the {} half\".format(shi))\n","                # prepare the output image\n","                img_raw_shi = img_draw[new_height_hf*(shi-1):new_height_hf*shi,:,:]\n","                #\n","                samples = img_in[:,new_height_hf*(shi-1):new_height_hf*shi,:].unsqueeze(0).to(device)\n","                # run inference\n","                outputs = model(samples)\n","\n","                outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]\n","\n","                outputs_points = outputs['pred_points'][0]\n","\n","                # filter the predictions\n","                # 0.5 is used by default\n","                points = outputs_points[outputs_scores > threshold].detach().cpu().numpy()#.tolist()\n","                if points.shape[0]< 10000 and points.shape[0] != 0:\n","                    #print(\"doing clustering\")\n","                    cutoff = 500/points.shape[0]\n","                    if cutoff<20:\n","                        cutoff = 20\n","                    components = nx.connected_components(\n","                        nx.from_edgelist(\n","                            (i, j) for i, js in enumerate(\n","                                spatial.KDTree(points).query_ball_point(points, cutoff)\n","                            )\n","                            for j in js\n","                        )\n","                    )\n","\n","                    clusters = {j: i for i, js in enumerate(components) for j in js}\n","\n","                    # reorganize the points to the order of clusters \n","                    points_reo = np.zeros(points.shape)\n","                    i = 0\n","                    for key in clusters.keys():\n","                        #print(key)\n","                        points_reo[i,:] = points[key,:]\n","                        i+=1\n","                    # points_n has the same order as clusters\n","                    res = [clusters[key] for key in clusters.keys()]\n","                    res_n = np.array(res).reshape(-1,1)\n","\n","                    points_n = []\n","                    for i in np.unique(res_n):\n","                        tmp = points_reo[np.where(res_n[:,0] == i)]\n","                        points_n.append( [np.mean(tmp[:,0]), np.mean(tmp[:,1])])\n","                else:\n","                    points_n = points.tolist()\n","                #\n","                if shi ==1:\n","                    points_bf_sum = np.array(points)\n","                    points_af_sum = np.array(points_n)\n","                    print(\"points_af_sum {}\".format(points_af_sum.shape))\n","                else:\n","                    points_bf_sum = np.concatenate((points_bf_sum, np.array(points)), 0)\n","                    points_af_sum = np.concatenate((points_af_sum, np.array(points_n)), 0)\n","                \n","                # draw the predictions\n","                alpha = 0.5\n","                #. before \n","                size = 6\n","                img_to_draw_before_in = cv2.cvtColor(np.array(img_raw_shi), cv2.COLOR_RGB2BGR)\n","                img_to_draw_before_in_n = img_to_draw_before_in.copy()\n","                for p in points:\n","                    img_to_draw_before_in_n = cv2.circle(img_to_draw_before_in_n, (int(p[0]), int(p[1])), size, (0, 0, 255), -1)\n","                img_to_draw_before_in_nn = cv2.addWeighted(img_to_draw_before_in_n, alpha, img_to_draw_before_in, 1 - alpha, 0)\n","                # save the visualized image\n","                img_to_draw_before[new_height_hf*(shi-1):new_height_hf*shi,:,:] = img_to_draw_before_in_nn\n","                #. after  \n","                #size = 6\n","                img_to_draw_after_in = cv2.cvtColor(np.array(img_raw_shi), cv2.COLOR_RGB2BGR)\n","                img_to_draw_after_in_n = img_to_draw_after_in.copy()\n","                for p in points_n:\n","                    img_to_draw_after_in_n = cv2.circle(img_to_draw_after_in_n, (int(p[0]), int(p[1])), size, (0, 0, 255), -1)\n","                #\n","                img_to_draw_after_in_nn = cv2.addWeighted(img_to_draw_after_in_n, alpha, img_to_draw_after_in, 1 - alpha, 0)\n","                img_to_draw_after[new_height_hf*(shi-1):new_height_hf*shi,:,:] = img_to_draw_after_in_nn\n","            #\n","            cv2.imwrite(os.path.join(args.vis_dir, img_name_pred_before), img_to_draw_before[:height,:width,:])\n","            # save the visualized image\n","            cv2.imwrite(os.path.join(args.vis_dir, img_name_pred_after), img_to_draw_after[:height,:width,:])\n","            #\n","            #predict_cnt = int((outputs_scores > threshold).sum())\n","            points_bf_sum = points_bf_sum.tolist()\n","            points_af_sum = points_af_sum.tolist()\n","            predict_cnt_before = len(points_bf_sum)\n","            predict_cnt_after = len(points_af_sum)\n","\n","        else:\n","            samples = img_in.unsqueeze(0).to(device)\n","            # run inference\n","            outputs = model(samples)\n","\n","            outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]\n","\n","            outputs_points = outputs['pred_points'][0]\n","\n","            # filter the predictions\n","            # 0.5 is used by default\n","            points = outputs_points[outputs_scores > threshold].detach().cpu().numpy()#.tolist()\n","            if points.shape[0]< 10000 and points.shape[0] != 0:\n","                #print(\"doing clustering\")\n","                cutoff = 500/points.shape[0]\n","                if cutoff<20:\n","                    cutoff = 20\n","                components = nx.connected_components(\n","                    nx.from_edgelist(\n","                        (i, j) for i, js in enumerate(\n","                            spatial.KDTree(points).query_ball_point(points, cutoff)\n","                        )\n","                        for j in js\n","                    )\n","                )\n","\n","                clusters = {j: i for i, js in enumerate(components) for j in js}\n","\n","                # reorganize the points to the order of clusters \n","                points_reo = np.zeros(points.shape)\n","                i = 0\n","                for key in clusters.keys():\n","                    #print(key)\n","                    points_reo[i,:] = points[key,:]\n","                    i+=1\n","                # points_n has the same order as clusters\n","                res = [clusters[key] for key in clusters.keys()]\n","                res_n = np.array(res).reshape(-1,1)\n","\n","                points_n = []\n","                for i in np.unique(res_n):\n","                    tmp = points_reo[np.where(res_n[:,0] == i)]\n","                    points_n.append( [np.mean(tmp[:,0]), np.mean(tmp[:,1])])\n","            else:\n","                points_n = points.tolist()\n","            # calculate the distance and find the center of the too close points\n","\n","            #predict_cnt = int((outputs_scores > threshold).sum())\n","            predict_cnt_before = len(points)\n","            predict_cnt_after = len(points_n)\n","            #\n","            print(\"Number of seeds before = {}\".format(predict_cnt_before))\n","            print(\"Number of seeds after = {}\".format(predict_cnt_after))\n","            # draw the predictions\n","            alpha = 0.5\n","            #. before \n","            size = 6\n","            #\n","            img_to_draw_before = cv2.cvtColor(np.array(img_raw), cv2.COLOR_RGB2BGR)\n","            img_to_draw_before_in_x = img_to_draw_before.copy()\n","            for p in points:\n","                img_to_draw_before_in_x = cv2.circle(img_to_draw_before_in_x, (int(p[0]), int(p[1])), size, (0, 0, 255), -1)\n","            img_to_draw_before_in_xx = cv2.addWeighted(img_to_draw_before_in_x, alpha, img_to_draw_before, 1 - alpha, 0)\n","            # save the visualized image\n","            cv2.imwrite(os.path.join(args.vis_dir, img_name_pred_before), img_to_draw_before_in_xx[:height,:width,:])\n","            #. after  \n","            #size = 6\n","            img_to_draw_after = cv2.cvtColor(np.array(img_raw), cv2.COLOR_RGB2BGR)\n","            img_to_draw_after_in_x = img_to_draw_after.copy()\n","            for p in points_n:\n","                img_to_draw_after_in_x = cv2.circle(img_to_draw_after_in_x, (int(p[0]), int(p[1])), size, (0, 0, 255), -1)\n","            img_to_draw_after_in_xx = cv2.addWeighted(img_to_draw_after_in_x, alpha, img_to_draw_after, 1 - alpha, 0)\n","            # save the visualized image\n","            cv2.imwrite(os.path.join(args.vis_dir, img_name_pred_after), img_to_draw_after_in_xx[:height,:width,:])\n","            #\n","        print(\"Number of seeds before = {}\".format(predict_cnt_before))\n","        print(\"Number of seeds after = {}\".format(predict_cnt_after))\n","        # save the detected pod number\n","        loss_csv.write('{},{},{},{}\\n'.format(img_name_pred_before.split(\".\")[0], predict_cnt_before, img_name_pred_after.split(\".\")[0], predict_cnt_after))\n","        loss_csv.flush()  \n","    loss_csv.close\n"],"metadata":{"id":"ssweK6QY6_vT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####On big infield images"],"metadata":{"id":"urXEOss_7Ual"}},{"cell_type":"code","source":["# threshold for evaluation\n","threshold = 0.5 #args.threshold\n","# to apply post processing or not: if Filter = 10000, the filter is applied\n","\n","#\n","# load trained model\n","if args.resume is not None:\n","    checkpoint = torch.load(args.resume, map_location='cpu')\n","    model.load_state_dict(checkpoint['model'])\n","#\n","model.eval()\n","#\n","step = 0\n","epoch = 0\n","###\n","# create the pre-processing transform\n","transform = standard_transforms.Compose([\n","    standard_transforms.ToTensor(), \n","    standard_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# set your image path here\n","folder_path = \"/content/drive/MyDrive/soybean_1118selection_annotated/\"\n","folder_sub = os.listdir(folder_path)\n","#\n","# save the number of detected pods\n","csv_name = (args.vis_dir).split(\"/\")[-1]\n","loss_csv = open(os.path.join(args.vis_dir, '{}_seed_counting_{}.csv'.format(csv_name, 'final')), 'w+')\n","#\n","img_ct = 0\n","for sub in range(len(folder_sub)):\n","    #\n","    folder_sub_s = folder_sub[sub]\n","    #\n","    folder_path_s = os.path.join(folder_path, folder_sub_s)\n","    #\n","    img_list = glob.glob(os.path.join(folder_path_s,'*.JPG'))#[:3]\n","    #\n","    for img_path_ij in img_list:\n","        print(\"image path = {}\".format(img_path_ij))\n","        #\n","        img_name = img_path_ij.split(\"/\")[-1]\n","        #\n","        img_name_pred_before = img_name.replace(\".png\", '_pred_bf.png')\n","        img_name_pred_after = img_name.replace(\".png\", '_pred_af.png')\n","        # load the images\n","        img_00 = cv2.imread(img_path_ij)\n","        img_0 = cv2.resize(img_00, (int(img_00.shape[1]/2), int(img_00.shape[0]/2)), interpolation = cv2.INTER_AREA)\n","        # this is only for drawing points\n","        #img_raw = Image.fromarray(cv2.cvtColor(img_0, cv2.COLOR_BGR2RGB))\n","        img_raw = cv2.cvtColor(img_0, cv2.COLOR_BGR2RGB)\n","        # pre-proccessing\n","        img = transform(img_raw)\n","        ##\n","        # round the size\n","        height, width = img_0.shape[:2]\n","        # get the new input image size suitable for VGG16 net\n","        new_width = (width // 128 +1) * 128\n","        new_height = (height // 128 + 1)* 128\n","        ##\n","        img_in = torch.zeros((3, new_height, new_width))\n","        img_in[:,:height,:width] = img\n","\n","        # divide the image into two to save memory\n","        print(\"new_height = {}\".format(new_height))\n","        # for out out image\n","        img_draw = (np.ones((new_height, new_width, 3))*255).astype(np.uint8)\n","        img_draw[:height, :width,:] = img_raw\n","        #\n","        img_to_draw_before = img_draw.copy() \n","        img_to_draw_after = img_draw.copy() \n","        #\n","        new_width_hf = int(new_width/4)\n","        new_height_hf = int(new_height/4)\n","        for shi in range(1,5):\n","            print(\"the {} half\".format(shi))\n","            # prepare the output image\n","            img_raw_shi = img_draw[:,new_width_hf*(shi-1):new_width_hf*shi,:]\n","            #\n","            samples = img_in[:,:,new_width_hf*(shi-1):new_width_hf*shi].unsqueeze(0).to(device)\n","            # run inference\n","            outputs = model(samples)\n","\n","            outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]\n","\n","            outputs_points = outputs['pred_points'][0]\n","\n","            # filter the predictions\n","            # 0.5 is used by default\n","            points = outputs_points[outputs_scores > threshold].detach().cpu().numpy()#.tolist()\n","            if points.shape[0]< 100000 and points.shape[0] != 0:\n","                #print(\"doing clustering\")\n","                cutoff = 500/points.shape[0]\n","                if cutoff<10:\n","                    cutoff = 10\n","                components = nx.connected_components(\n","                    nx.from_edgelist(\n","                        (i, j) for i, js in enumerate(\n","                            spatial.KDTree(points).query_ball_point(points, cutoff)\n","                        )\n","                        for j in js\n","                    )\n","                )\n","\n","                clusters = {j: i for i, js in enumerate(components) for j in js}\n","\n","                # reorganize the points to the order of clusters \n","                points_reo = np.zeros(points.shape)\n","                i = 0\n","                for key in clusters.keys():\n","                    #print(key)\n","                    points_reo[i,:] = points[key,:]\n","                    i+=1\n","                # points_n has the same order as clusters\n","                res = [clusters[key] for key in clusters.keys()]\n","                res_n = np.array(res).reshape(-1,1)\n","\n","                points_n = []\n","                for i in np.unique(res_n):\n","                    tmp = points_reo[np.where(res_n[:,0] == i)]\n","                    points_n.append( [np.mean(tmp[:,0]), np.mean(tmp[:,1])])\n","            else:\n","                points_n = points.tolist()\n","            #\n","            if points_n:\n","                    if shi ==1:\n","                        points_bf_sum = np.array(points)\n","                        points_af_sum = np.array(points_n)\n","                        print(\"points_af_sum {}\".format(points_af_sum.shape))\n","                    else:\n","                        points_bf_sum = np.concatenate((points_bf_sum, np.array(points)), 0)\n","                        points_af_sum = np.concatenate((points_af_sum, np.array(points_n)), 0)\n","            # draw the predictions\n","            alpha = 0.6\n","            #. before \n","            size = 5\n","            img_to_draw_before_in = cv2.cvtColor(np.array(img_raw_shi), cv2.COLOR_RGB2BGR)\n","            img_to_draw_before_in_n = img_to_draw_before_in.copy()\n","            for p in points:\n","                img_to_draw_before_in_n = cv2.circle(img_to_draw_before_in_n, (int(p[0]), int(p[1])), size, (0, 0, 255), -1)\n","            img_to_draw_before_in_nn = cv2.addWeighted(img_to_draw_before_in_n, alpha, img_to_draw_before_in, 1 - alpha, 0)\n","            # save the visualized image\n","            img_to_draw_before[:,new_width_hf*(shi-1):new_width_hf*shi,:] = img_to_draw_before_in_nn\n","            #. after  \n","            #size = 4\n","            img_to_draw_after_in = cv2.cvtColor(np.array(img_raw_shi), cv2.COLOR_RGB2BGR)\n","            img_to_draw_after_in_n = img_to_draw_after_in.copy()\n","            for p in points_n:\n","                img_to_draw_after_in_n = cv2.circle(img_to_draw_after_in_n, (int(p[0]), int(p[1])), size, (0, 0, 255), -1)\n","            #\n","            img_to_draw_after_in_nn = cv2.addWeighted(img_to_draw_after_in_n, alpha, img_to_draw_after_in, 1 - alpha, 0)\n","            img_to_draw_after[:,new_width_hf*(shi-1):new_width_hf*shi,:] = img_to_draw_after_in_nn\n","        #\n","        cv2.imwrite(os.path.join(args.vis_dir, img_name_pred_before), img_to_draw_before[:height,:width,:])\n","        # save the visualized image\n","        cv2.imwrite(os.path.join(args.vis_dir, img_name_pred_after), img_to_draw_after[:height,:width,:])\n","        #\n","        #predict_cnt = int((outputs_scores > threshold).sum())\n","        points_bf_sum = points_bf_sum.tolist()\n","        points_af_sum = points_af_sum.tolist()\n","        predict_cnt_before = len(points_bf_sum)\n","        predict_cnt_after = len(points_af_sum)\n","        #\n","        print(\"Number of seeds before = {}\".format(predict_cnt_before))\n","        print(\"Number of seeds after = {}\".format(predict_cnt_after))\n","        # save the detected pod number\n","        loss_csv.write('{},{},{},{}\\n'.format(img_name_pred_before.split(\".\")[0], predict_cnt_before, img_name_pred_after.split(\".\")[0], predict_cnt_after))\n","        loss_csv.flush()  \n","        #\n","        print(\"saving {}th image {}\".format(img_ct, img_path_ij))\n","        img_ct +=1\n","loss_csv.close\n"],"metadata":{"id":"i3Su3D1f6_xr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"raw-srBe6_0q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tU9zDmkU6_5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0k5MUxzRArWh"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[{"file_id":"11QaSulSlEV4fnYEcS9zQ2W6TbNA3QwVq","timestamp":1667981629049},{"file_id":"1IunanfU2NJyt4Pd4XuwE5Ky1_K7O3rP_","timestamp":1667909260114}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}